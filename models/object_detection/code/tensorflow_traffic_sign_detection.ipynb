{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_traffic_sign_detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karavdin/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQCnYPVDrsgx"
      },
      "source": [
        "# Training a Raspberry Pi to Detect Traffic Signs and People in Real Time\n",
        "\n",
        "This is tutorial is based on Chengwei's excellent Tutorial and Colab Notebook on [\"How to train an object detection model easy for free\"](https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/).   My twist on his tutorial is that I need to run my model on a Raspberry Pi with live video feed.  As the Raspberry Pi is fairly limited on CPU power and can only run object detection at 1-2 FPS (frames/sec), I have purchased the newly release $75 Google's [EdgeTPU USB Accelarator](https://coral.withgoogle.com/products/accelerator), which can detect objects at 12 FPS, which is sufficient for real time work.  After doing the transfer learning from one of the object detection models using our own images, last few steps of the colab deals with how to convert a trained model to a model file that can be consumed by an Edge TPU, namely, the final `mymodel_quantized_edgetpu.tflite` file.  \n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*_jABdMfUVcyPdi5b3zlfVg.jpeg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOThFKokSmU"
      },
      "source": [
        "# Section 1: Mount Google drive\n",
        "Mount my Google Drive and save modeling output files (`.ckpt`)  there, so that it won't be wiped out when colab Virtual Machine restarts.  It has an idle timeout of 90 min, and maximum daily usage of 12 hours.\n",
        "\n",
        "Google will ask for an authenticate code when you run the following code, just follow the link in the output and allow access.   You can put the `model_dir` anywhere in your google drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DEe8d3SAZRvH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA1y7NMxFT_B",
        "outputId": "7b976af8-c17e-4838-a594-092c582402fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "model_dir = '/content/gdrive/My Drive/Colab Notebooks/DeepCar_Training'\n",
        "#!rm -rf '{model_dir}'\n",
        "#os.makedirs(model_dir, exist_ok=True)\n",
        "!ls -ltra '{model_dir}'/.."
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "total 27181\n",
            "-rw------- 1 root root   79210 Jun 30  2019  my_em_assignment.ipynb\n",
            "-rw------- 1 root root  395675 Jul  9  2019 'my mcmc_assignment.ipynb'\n",
            "-rw------- 1 root root  206381 Jul 24  2019  my_Vae_assignment.ipynb\n",
            "-rw------- 1 root root  243880 Jul 28  2019  my_gp_assignment.ipynb\n",
            "-rw------- 1 root root  968824 Jul 29  2019  my_finding_suspect.ipynb\n",
            "-rw------- 1 root root     287 Aug 22  2019  Untitled0.ipynb\n",
            "-rw------- 1 root root 1698449 Aug 22  2019 'Bike Sharing Prediction.ipynb'\n",
            "-rw------- 1 root root     287 Aug 27  2019  Untitled1.ipynb\n",
            "-rw------- 1 root root  781572 Aug 28  2019 'Trip waiting time prediction.ipynb'\n",
            "-rw------- 1 root root   19520 Sep 10  2019 'Data Engineer Challenge Swarovski.ipynb'\n",
            "-rw------- 1 root root     269 Sep 22  2019  Untitled2.ipynb\n",
            "-rw------- 1 root root  190302 Sep 24  2019  Prototype2.ipynb\n",
            "-rw------- 1 root root  128216 Oct 30  2019 'My Copy of Exercise_Generative_Models_Problem.ipynb'\n",
            "-rw------- 1 root root  268926 Oct 30  2019 'My Copy of Exercise_Generative_Models_Solution.ipynb'\n",
            "-rw------- 1 root root   42798 Nov 10  2019 'Copy of Course 1 - Part 4 - Lesson 2 - Notebook.ipynb'\n",
            "-rw------- 1 root root   39200 Nov 10  2019 'Copy of Course 1 - Part 6 - Lesson 2 - Notebook.ipynb'\n",
            "-rw------- 1 root root 2045038 Nov 10  2019 'Copy of Horse-or-Human-NoValidation.ipynb'\n",
            "-rw------- 1 root root 1365097 Nov 11  2019 'Copy of Course 2 - Part 4 - Lesson 2 - Notebook.ipynb'\n",
            "-rw------- 1 root root   33938 Nov 16  2019 'Copy of Transfer Learning.ipynb'\n",
            "-rw------- 1 root root   13584 Nov 24  2019 'Copy of S+P Week 2 Lesson 1 (preparing features and lables for time series).ipynb'\n",
            "-rw------- 1 root root  167061 Nov 25  2019 'Copy of S+P Week 3 Lesson 4 - LSTM.ipynb'\n",
            "-rw------- 1 root root   36836 Jun 20  2020  Etsy_Geometry_of_Time_Stats_2020.ipynb\n",
            "-rw------- 1 root root   36917 Jun 20  2020 'Copy of Etsy_Geometry_of_Time_Stats_2020.ipynb'\n",
            "-rw------- 1 root root    4388 Aug 15  2021  Untitled3.ipynb\n",
            "-rw------- 1 root root     410 Dec 24  2021  Yandex_Praktikum_DS_DA.ipynb\n",
            "-rw------- 1 root root   14119 Dec 24  2021  Task1.ipynb\n",
            "-rw------- 1 root root 6591516 Dec 28  2021  shap-implementation-1d2cca5e12634c4f96fb56e6dcee44f1.ipynb\n",
            "-rw------- 1 root root 3060145 Dec 28  2021 'My Version Explainable AI Graded Hands-on Exercise.ipynb'\n",
            "-rw------- 1 root root 1917384 Jan  5  2022  etsy_krabbeldecke.ipynb\n",
            "-rw------- 1 root root 3384812 Jan  5  2022  etsy_countries.ipynb\n",
            "-rw------- 1 root root 3947369 Jan  6  2022  etsy_listings.ipynb\n",
            "-rw------- 1 root root   80815 Nov 19 12:40 \"Anastasia's Copy of Tweet Emotion Recognition - Learner.ipynb\"\n",
            "drwx------ 2 root root    4096 Jan 22 11:00  DeepCar_Training\n",
            "-rw------- 1 root root   57225 Jan 22 11:01 'Kopie von tensorflow_traffic_sign_detection.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzxsJb3dpWq"
      },
      "source": [
        "# Section 2: Configs and Hyperparameters\n",
        "\n",
        "Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNXNQCjdniL"
      },
      "source": [
        "# If you forked the repository, you can replace the link.\n",
        "repo_url = 'https://github.com/karavdin/DeepPiCar'\n",
        "\n",
        "# Number of training steps.\n",
        "#num_steps = 1000  # 200000\n",
        "num_steps = 10  # 200000\n",
        "\n",
        "# Number of evaluation steps.\n",
        "#num_eval_steps = 50\n",
        "num_eval_steps = 5\n",
        "\n",
        "\n",
        "# model configs are from Model Zoo github: \n",
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
        "MODELS_CONFIG = {\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n",
        "    'ssd_mobilenet_v1_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n",
        "        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
        "        'batch_size': 12\n",
        "    },    \n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "    'ssd_mobilenet_v2_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "        'batch_size': 12\n",
        "    }\n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "# Note: for Edge TPU, you have to:\n",
        "# 1) start with a pretrained model from model zoo, such as above 4\n",
        "# 2) Must be a quantized model, which reduces the model size significantly\n",
        "selected_model = 'ssd_mobilenet_v2_quantized'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-YqZHUKv-Y"
      },
      "source": [
        "# Section 3: Set up Training Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_xVkoa4KjHO"
      },
      "source": [
        "## Clone the `DeepPiCar` repository or your fork."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxc3DmvLQF3z",
        "outputId": "9a150e13-6d75-408b-a0ab-deab212980c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "print('Pull it so that we have the latest code/data')\n",
        "!git pull"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'DeepPiCar' already exists and is not an empty directory.\n",
            "/content/DeepPiCar\n",
            "Pull it so that we have the latest code/data\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 6 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), 5.74 KiB | 839.00 KiB/s, done.\n",
            "From https://github.com/karavdin/DeepPiCar\n",
            "   eb08c61..73eca6f  master     -> origin/master\n",
            "Updating eb08c61..73eca6f\n",
            "Fast-forward\n",
            " .../code/tensorflow_traffic_sign_detection.ipynb   | 504 \u001b[32m+++++++++++++++\u001b[m\u001b[31m------\u001b[m\n",
            " 1 file changed, 366 insertions(+), 138 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8__uNS8-ns"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "sBTP_y94F0hH"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "# !pip install tf_slim\n",
        "# !pip install tensorflow_io\n",
        "# !pip install tensorflow-addons\n",
        "!pip install --ignore-installed --upgrade tensorflow==2.7.0"
      ],
      "metadata": {
        "id": "KGiQGSg5ByEK",
        "outputId": "a89b2c54-1cce-48d6-f214-9483a7278e05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Using cached tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
            "Collecting libclang>=9.0.1\n",
            "  Using cached libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
            "Collecting tensorboard~=2.6\n",
            "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Using cached wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting wheel<1.0,>=0.32.0\n",
            "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Using cached protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "Collecting numpy>=1.14.5\n",
            "  Using cached numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.30.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting six>=1.12.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Using cached h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Using cached grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "Collecting absl-py>=0.4.0\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Using cached google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting setuptools>=41.0.0\n",
            "  Using cached setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Using cached cachetools-5.2.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Using cached importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Using cached MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, charset-normalizer, zipp, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, six, setuptools, rsa, pyasn1-modules, protobuf, oauthlib, numpy, MarkupSafe, idna, grpcio, gast, certifi, cachetools, absl-py, werkzeug, requests, opt-einsum, keras-preprocessing, importlib-metadata, h5py, google-pasta, google-auth, astunparse, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "tf-models-official 2.11.3 requires tensorflow~=2.11.0, but you have tensorflow 2.7.0 which is incompatible.\n",
            "tensorflow-text 2.11.0 requires tensorflow<2.12,>=2.11.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.7.0 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.2.2 which is incompatible.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 66.1.1 which is incompatible.\n",
            "apache-beam 2.44.0 requires numpy<1.23.0,>=1.14.3, but you have numpy 1.24.1 which is incompatible.\n",
            "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.2.1 certifi-2022.12.7 charset-normalizer-3.0.1 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 h5py-3.7.0 idna-3.4 importlib-metadata-6.0.0 keras-2.11.0 keras-preprocessing-1.1.2 libclang-15.0.6.1 markdown-3.4.1 numpy-1.24.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.20.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 setuptools-66.1.1 six-1.16.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0 typing-extensions-4.4.0 urllib3-1.26.14 werkzeug-2.2.2 wheel-0.38.4 wrapt-1.14.1 zipp-3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"--- Tensorflow version --- \\n\", tf.__version__)\n",
        "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
      ],
      "metadata": {
        "id": "OlUNke4GDMYI",
        "outputId": "2f03cc52-8668-46ac-be29-0ec6aaa85cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tensorflow version --- \n",
            " 2.5.0\n",
            "tf.Tensor(796.9023, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecpHEnka8Kix",
        "outputId": "8b73a59d-81fe-4091-fe36-fcd5804942f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q cython contextlib2 pillow lxml matplotlib\n",
        "#!pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
        "\n",
        "\n",
        "# !git clone https://github.com/cocodataset/cocoapi.git\n",
        "# %cd cocoapi/PythonAPI\n",
        "# !make\n",
        "# !cp -r pycocotools /content/models/research/\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install --use-feature=2020-resolver .\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/:/content/models/'\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/models\")\n",
        "\n",
        "#!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "/content/models/research\n",
            "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: avro-python3 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.10.2)\n",
            "Requirement already satisfied: apache-beam in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.44.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (4.9.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.29.33)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.0)\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Requirement already satisfied: tf-models-official>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.11.3)\n",
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.30.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: sacrebleu<=2.2.0 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.24.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.1.97)\n",
            "Requirement already satisfied: pyyaml<6.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.70.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.7.0.68)\n",
            "Requirement already satisfied: tensorflow-text~=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: tensorflow~=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.7.3)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.19.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.8.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.2.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Collecting numpy>=1.17\n",
            "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n",
            "Requirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.22.2)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.28.2)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (3.13.0)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (3.8.5)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.3.1.1)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.51.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.7.0)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (4.4.0)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.18)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.19.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.30.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_io->object-detection==0.1) (0.30.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.1.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.8/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.12.7)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.4)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.0)\n",
            "Collecting protobuf<4,>3.12.2\n",
            "  Using cached protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (23.1.21)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (66.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.12.0)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.58.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.2.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=21919971 sha256=d78066a4521af4d1a05191f8d80cbb106b9859f546826687868bd106a8357c36\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_fmh_4bn/wheels/7d/96/c1/072a751379735e8dfdada1def1c62a89afb3cc45654fd6fd28\n",
            "Successfully built object-detection\n",
            "Installing collected packages: protobuf, numpy, object-detection\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.1\n",
            "    Uninstalling numpy-1.24.1:\n",
            "      Successfully uninstalled numpy-1.24.1\n",
            "  Attempting uninstall: object-detection\n",
            "    Found existing installation: object-detection 0.1\n",
            "    Can't uninstall 'object-detection'. No files were found to uninstall.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 66.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.22.4 object-detection-0.1 protobuf-3.19.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "metadata": {
        "id": "DW8sd8Y_B3W7",
        "outputId": "626274bc-d473-4bae-c8e0-383091ff0794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-22 15:26:17.030649: E tensorflow/core/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\n",
            "Traceback (most recent call last):\n",
            "  File \"object_detection/builders/model_builder_tf2_test.py\", line 21, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 479, in <module>\n",
            "    keras._load()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\n",
            "    module = importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/__init__.py\", line 25, in <module>\n",
            "    from keras import models\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/models/__init__.py\", line 18, in <module>\n",
            "    from keras.engine.functional import Functional\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 25, in <module>\n",
            "    from keras.engine import base_layer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 43, in <module>\n",
            "    from keras.mixed_precision import loss_scale_optimizer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\", line 20, in <module>\n",
            "    from keras.optimizer_v2 import optimizer_v2\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 36, in <module>\n",
            "    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 360, in __init__\n",
            "    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 135, in __init__\n",
            "    self._metric = self._metric_methods[self._label_length].create(*args)\n",
            "tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny"
      },
      "source": [
        "## Prepare `tfrecord` files\n",
        "\n",
        "Use the following scripts to generate the `tfrecord` files.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezGDABRXXhPP",
        "outputId": "fa2b91c5-8ba4-43f9-9c6d-5ddeef2c1e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd {repo_dir_path}/models/object_detection\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python code/xml_to_csv.py -i data/images_hh/train -o data/annotations_hh/train_labels.csv -l data/annotations_hh\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python code/xml_to_csv.py -i data/images_hh/test -o data/annotations_hh/test_labels.csv\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepPiCar/models/object_detection\n",
            "Successfully converted xml to csv.\n",
            "Generate `data/annotations_hh/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove duplicates"
      ],
      "metadata": {
        "id": "UYMsl6vhPy29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "u5yq_Xv7VOhy",
        "outputId": "a9c2dc1a-7418-4a3b-c221-99e97af9d2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code  data  docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_labels = pd.read_csv('data/annotations_hh/train_labels.csv')\n",
        "\n",
        "# df_train_labels['class'] = df_train_labels['class'].replace(['Babuschka'], 'Babushka')\n",
        "# df_train_labels['class'] = df_train_labels['class'].replace(['Capitane'], 'Capitan')\n",
        "print(df_train_labels['class'].unique())\n",
        "# df_train_labels.to_csv('data/annotations_hh/train_labels.csv')"
      ],
      "metadata": {
        "id": "dtue93YVP3vP",
        "outputId": "d4a3707e-a070-4af0-fc8b-12dbd36ad15c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nastya' 'Sheila' 'Babushka' 'Cam' 'Capitan' 'Phil' 'Bob' 'clock'\n",
            " 'e-bike station' 'ukulele' 'bike' 'Ukulele' 'disko']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_labels = pd.read_csv('data/annotations_hh/test_labels.csv')\n",
        "\n",
        "# df_test_labels['class'] = df_test_labels['class'].replace(['Babuschka'], 'Babushka')\n",
        "# df_test_labels['class'] = df_test_labels['class'].replace(['Capitane'], 'Capitan')\n",
        "print(df_test_labels['class'].unique())\n",
        "# df_test_labels.to_csv('data/annotations_hh/test_labels.csv')"
      ],
      "metadata": {
        "id": "-MLvAaiWRo1-",
        "outputId": "364056d0-f782-4133-837d-3e47cefd894d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bob' 'Babushka' 'bike' 'Phil' 'e-bike station' 'clock' 'Nastya' 'Cam'\n",
            " 'Capitan' 'Sheila' 'ukulele']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate `train.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations_hh/train_labels.csv --output_path=data/annotations_hh/train.record --img_path=data/images_hh/train --label_map data/annotations_hh/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations_hh/test_labels.csv --output_path=data/annotations_hh/test.record --img_path=data/images_hh/test --label_map data/annotations_hh/label_map.pbtxt"
      ],
      "metadata": {
        "id": "JiWqYfQ7P6S8",
        "outputId": "f6df3fdd-be66-4308-fbca-bfa5f385cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-22 15:26:25.251008: E tensorflow/core/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\n",
            "Traceback (most recent call last):\n",
            "  File \"code/generate_tfrecord.py\", line 19, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 479, in <module>\n",
            "    keras._load()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\n",
            "    module = importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/__init__.py\", line 25, in <module>\n",
            "    from keras import models\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/models/__init__.py\", line 18, in <module>\n",
            "    from keras.engine.functional import Functional\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 25, in <module>\n",
            "    from keras.engine import base_layer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 43, in <module>\n",
            "    from keras.mixed_precision import loss_scale_optimizer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\", line 20, in <module>\n",
            "    from keras.optimizer_v2 import optimizer_v2\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 36, in <module>\n",
            "    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 360, in __init__\n",
            "    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 135, in __init__\n",
            "    self._metric = self._metric_methods[self._label_length].create(*args)\n",
            "tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\n",
            "2023-01-22 15:26:32.670817: E tensorflow/core/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\n",
            "Traceback (most recent call last):\n",
            "  File \"code/generate_tfrecord.py\", line 19, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 479, in <module>\n",
            "    keras._load()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\n",
            "    module = importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/__init__.py\", line 25, in <module>\n",
            "    from keras import models\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/models/__init__.py\", line 18, in <module>\n",
            "    from keras.engine.functional import Functional\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 25, in <module>\n",
            "    from keras.engine import base_layer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 43, in <module>\n",
            "    from keras.mixed_precision import loss_scale_optimizer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\", line 20, in <module>\n",
            "    from keras.optimizer_v2 import optimizer_v2\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 36, in <module>\n",
            "    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 360, in __init__\n",
            "    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 135, in __init__\n",
            "    self._metric = self._metric_methods[self._label_length].create(*args)\n",
            "tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgd-fzAIkZlV"
      },
      "source": [
        "test_record_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/test.record'\n",
        "train_record_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/train.record'\n",
        "label_map_pbtxt_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/label_map.pbtxt'"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8otnfyCYdG",
        "outputId": "657fd56a-722a-4c0f-bbd3-c5f17668a8b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat data/annotations_hh/test_labels.csv"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filename,width,height,class,xmin,ymin,xmax,ymax\n",
            "test_47.jpg,2592,1944,Bob,326,921,459,1201\n",
            "test_47.jpg,2592,1944,Babushka,737,941,944,1270\n",
            "test_47.jpg,2592,1944,bike,939,986,997,1103\n",
            "test_47.jpg,2592,1944,Phil,1101,743,1295,1021\n",
            "test_47.jpg,2592,1944,e-bike station,1106,323,1895,1588\n",
            "test_47.jpg,2592,1944,bike,2166,1059,2335,1361\n",
            "test_47.jpg,2592,1944,clock,2012,763,2270,1190\n",
            "test_22.jpg,2592,1944,bike,419,983,615,1194\n",
            "test_22.jpg,2592,1944,Babushka,881,963,1070,1286\n",
            "test_22.jpg,2592,1944,clock,946,788,1175,972\n",
            "test_22.jpg,2592,1944,Nastya,1328,992,1415,1148\n",
            "test_22.jpg,2592,1944,Cam,1439,977,1624,1337\n",
            "test_22.jpg,2592,1944,Capitan,1668,999,1777,1212\n",
            "test_22.jpg,2592,1944,Sheila,1937,1037,2146,1512\n",
            "test_22.jpg,2592,1944,Phil,1895,706,2126,1034\n",
            "test_23.jpg,2592,1944,bike,261,988,675,1568\n",
            "test_23.jpg,2592,1944,Babushka,681,923,830,1237\n",
            "test_23.jpg,2592,1944,Sheila,1375,999,1579,1323\n",
            "test_23.jpg,2592,1944,Capitan,1650,994,1784,1212\n",
            "test_23.jpg,2592,1944,Sheila,1944,1032,2144,1532\n",
            "test_23.jpg,2592,1944,clock,941,781,1175,1092\n",
            "test_23.jpg,2592,1944,Nastya,1344,990,1404,1132\n",
            "test_23.jpg,2592,1944,e-bike station,1684,861,1730,999\n",
            "test_23.jpg,2592,1944,Phil,1899,697,2106,1023\n",
            "test_6.jpg,2592,1944,Nastya,1161,984,1227,1091\n",
            "test_6.jpg,2592,1944,ukulele,1198,1006,1222,1082\n",
            "test_6.jpg,2592,1944,bike,1333,1046,1478,1270\n",
            "test_6.jpg,2592,1944,Sheila,1696,970,1904,1376\n",
            "test_6.jpg,2592,1944,Capitan,1919,1005,2042,1226\n",
            "test_6.jpg,2592,1944,Phil,2094,722,2361,1223\n",
            "test_6.jpg,2592,1944,Bob,2370,1035,2586,1378\n",
            "test_6.jpg,2592,1944,clock,1779,812,2048,1137\n",
            "test_17.jpg,2592,1944,bike,197,974,481,1423\n",
            "test_17.jpg,2592,1944,Babushka,866,937,1135,1323\n",
            "test_17.jpg,2592,1944,Nastya,1332,968,1424,1152\n",
            "test_17.jpg,2592,1944,Cam,1395,979,1646,1352\n",
            "test_17.jpg,2592,1944,Capitan,1637,990,1788,1221\n",
            "test_17.jpg,2592,1944,Sheila,1984,1023,2230,1479\n",
            "test_17.jpg,2592,1944,Bob,2284,1017,2501,1372\n",
            "test_17.jpg,2592,1944,Phil,1881,690,2179,1054\n",
            "test_37.jpg,2592,1944,clock,10,454,304,1114\n",
            "test_37.jpg,2592,1944,bike,121,948,299,1270\n",
            "test_37.jpg,2592,1944,Babushka,675,954,824,1250\n",
            "test_37.jpg,2592,1944,Nastya,812,974,870,1123\n",
            "test_37.jpg,2592,1944,e-bike station,1264,663,1639,1290\n",
            "test_37.jpg,2592,1944,Sheila,1712,1028,1786,1197\n",
            "test_37.jpg,2592,1944,Capitan,1924,970,2221,1514\n",
            "test_37.jpg,2592,1944,Bob,2272,1034,2448,1341\n",
            "test_37.jpg,2592,1944,Phil,2019,746,2239,994\n",
            "test_44.jpg,2592,1944,Bob,337,926,468,1199\n",
            "test_44.jpg,2592,1944,Babushka,721,934,972,1266\n",
            "test_44.jpg,2592,1944,Phil,1097,746,1297,1119\n",
            "test_44.jpg,2592,1944,Cam,1359,981,1535,1243\n",
            "test_44.jpg,2592,1944,Sheila,1706,1023,1786,1181\n",
            "test_44.jpg,2592,1944,Capitan,1892,974,2232,1501\n",
            "test_44.jpg,2592,1944,bike,2404,1070,2592,1488\n",
            "test_10.jpg,2592,1944,Nastya,557,948,752,1286\n",
            "test_10.jpg,2592,1944,ukulele,650,992,730,1239\n",
            "test_10.jpg,2592,1944,Capitan,1044,959,1148,1108\n",
            "test_10.jpg,2592,1944,Babushka,1359,994,1441,1137\n",
            "test_10.jpg,2592,1944,Phil,641,653,911,1113\n",
            "test_10.jpg,2592,1944,Sheila,1707,1008,1796,1200\n",
            "test_10.jpg,2592,1944,bike,1832,1044,1900,1160\n",
            "test_10.jpg,2592,1944,e-bike station,1565,845,1723,1125\n",
            "test_10.jpg,2592,1944,clock,1797,809,2032,1137\n",
            "test_10.jpg,2592,1944,Bob,2332,1028,2589,1393\n",
            "test_2.jpg,2592,1944,Bob,2357,1019,2592,1399\n",
            "test_2.jpg,2592,1944,Phil,2099,694,2348,1219\n",
            "test_2.jpg,2592,1944,Capitan,1908,1012,2041,1237\n",
            "test_2.jpg,2592,1944,clock,1804,803,2044,1150\n",
            "test_2.jpg,2592,1944,Babushka,1495,992,1637,1146\n",
            "test_2.jpg,2592,1944,Cam,1312,994,1404,1112\n",
            "test_2.jpg,2592,1944,Sheila,1052,963,1161,1123\n",
            "test_2.jpg,2592,1944,Nastya,670,959,768,1159\n",
            "test_2.jpg,2592,1944,ukulele,695,1026,752,1128\n",
            "test_2.jpg,2592,1944,e-bike station,237,452,666,1294\n",
            "test_41.jpg,2592,1944,bike,128,948,290,1261\n",
            "test_41.jpg,2592,1944,Babushka,704,939,941,1274\n",
            "test_41.jpg,2592,1944,Phil,1092,741,1292,1123\n",
            "test_41.jpg,2592,1944,Cam,1357,994,1524,1246\n",
            "test_41.jpg,2592,1944,Sheila,1704,1030,1792,1194\n",
            "test_41.jpg,2592,1944,Capitan,1917,972,2204,1492\n",
            "test_41.jpg,2592,1944,Bob,2272,1034,2446,1332\n",
            "test_41.jpg,2592,1944,clock,2055,792,2279,1003\n",
            "test_28.jpg,2592,1944,Nastya,715,952,837,1197\n",
            "test_28.jpg,2592,1944,ukulele,761,1001,808,1172\n",
            "test_28.jpg,2592,1944,Capitan,935,943,1090,1286\n",
            "test_28.jpg,2592,1944,Cam,1235,992,1346,1172\n",
            "test_28.jpg,2592,1944,bike,1126,1003,1199,1121\n",
            "test_28.jpg,2592,1944,Sheila,1555,1012,1650,1194\n",
            "test_28.jpg,2592,1944,Bob,1686,994,1941,1481\n",
            "test_28.jpg,2592,1944,bike,1921,1070,2097,1281\n",
            "test_28.jpg,2592,1944,Babushka,2275,1019,2530,1459\n",
            "test_28.jpg,2592,1944,Phil,2008,746,2244,1083\n",
            "test_28.jpg,2592,1944,e-bike station,1628,868,1661,1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8"
      },
      "source": [
        "## Download base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDCj6ihgUMR",
        "outputId": "0a3e97e2-58be-4cbf-b077-9307f61a7042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGhvAObeiIix",
        "outputId": "a4b68571-c964-41cc-95ae-855f405f87b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/pretrained_model\n",
            "total 204M\n",
            "drwx------  2 303230 5000 4.0K Jan  4  2019 .\n",
            "drwxr-xr-x 25 root   root 4.0K Jan 22 15:26 ..\n",
            "-rw-------  1 303230 5000  93M Jan  4  2019 model.ckpt.data-00000-of-00001\n",
            "-rw-------  1 303230 5000  68K Jan  4  2019 model.ckpt.index\n",
            "-rw-------  1 303230 5000  20M Jan  4  2019 model.ckpt.meta\n",
            "-rw-------  1 303230 5000 4.3K Jan  4  2019 pipeline.config\n",
            "-rw-------  1 303230 5000  24M Jan  4  2019 tflite_graph.pb\n",
            "-rw-------  1 303230 5000  68M Jan  4  2019 tflite_graph.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnxlfRznPP3",
        "outputId": "2c6dff3b-5bfe-4b3d-9533-af687f3c2865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models/research/pretrained_model/model.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYW8J5JoLP4I"
      },
      "source": [
        "# Section 4: Transfer Learning Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD"
      },
      "source": [
        "## Configuring a Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhw7IdpLuiU"
      },
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG1nCNpUXcRU"
      },
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtCbLF2i0wI"
      },
      "source": [
        "import re\n",
        "\n",
        "# training pipeline file defines:\n",
        "# - pretrain model path\n",
        "# - the train/test sets\n",
        "# - ID to Label mapping and number of classes\n",
        "# - training batch size\n",
        "# - epochs to trains\n",
        "# - learning rate\n",
        "# - etc\n",
        "\n",
        "# note we just need to use a sample one, and make edits to it.\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test, we created earlier with our training/test sets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path: ID to label file\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps (Number of epochs to train)\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IH96bbydOWn",
        "outputId": "827caa02-92fb-4958-d55b-4cede492c04f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat {label_map_pbtxt_fname}"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item {\n",
            "    id: 1\n",
            "    name: 'Babushka'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 2\n",
            "    name: 'Bob'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 3\n",
            "    name: 'Cam'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 4\n",
            "    name: 'Capitan'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 5\n",
            "    name: 'Nastya'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 6\n",
            "    name: 'Phil'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 7\n",
            "    name: 'Sheila'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 8\n",
            "    name: 'Ukulele'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 9\n",
            "    name: 'bike'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 10\n",
            "    name: 'clock'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 11\n",
            "    name: 'disko'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 12\n",
            "    name: 'e-bike station'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 13\n",
            "    name: 'ukulele'\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0MEEanocn6",
        "outputId": "6dd493c2-84fd-4f1f-bbf1-bb733db48dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# look for num_classes: 13, since we have 13 different objects \n",
        "!cat {pipeline_fname}"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Quantized trained SSD with Mobilenet v2 on MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 13\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 12\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 10\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/train.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/test.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}\n",
            "\n",
            "graph_rewriter {\n",
            "  quantization {\n",
            "    delay: 48000\n",
            "    weight_bits: 8\n",
            "    activation_bits: 8\n",
            "  }\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23TECXvNezIF"
      },
      "source": [
        "## Run Tensorboard(Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2PZs-mSCmO",
        "outputId": "868c3bcd-0a4d-4f01-b317-ee8e972c29c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 15:26:40--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 54.237.133.81, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  3.43MB/s    in 5.0s    \n",
            "\n",
            "2023-01-22 15:26:45 (2.63 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8o6r1o5SC5M"
      },
      "source": [
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge1OX7gcSC7S"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5GSGxZNh8rp"
      },
      "source": [
        "### Get Tensorboard link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjhPT9iPSJ6T",
        "outputId": "3d148162-ae37-422d-b608-1a9a002686b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://62bd-34-125-107-1.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZc0RFcUjTa-"
      },
      "source": [
        "Now all inputs are set up, just train the model.   This process may take a few hours.   Since we are saving the model training results (model.ckpt-* files) in our google drive (a persistent storage that will survice the restart of our colab VM instance), we can safely leave and return a few hours later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm1Kkkymozf0"
      },
      "source": [
        "# #################### SEND ALERT EMAIL AT FINISH WITH GMAIL #####################\n",
        "# # To send email from Python from your google account, MUST \n",
        "# # 1) Enable less secure app\n",
        "# # https://myaccount.google.com/lesssecureapps\n",
        "# # 2) Disable Unlock Capcha\n",
        "# # https://accounts.google.com/b/0/DisplayUnlockCaptcha\n",
        "\n",
        "# import smtplib\n",
        "\n",
        "# def SendEmail(msg):\n",
        "#     with open('/content/gdrive/My Drive/Colab Notebooks/pw.txt') as file:\n",
        "#         data = file.readlines()\n",
        "        \n",
        "#     gmail_user = 'david.tian@gmail.com'  \n",
        "#     gmail_password = data[0]\n",
        "\n",
        "\n",
        "#     sent_from = gmail_user  \n",
        "#     to = ['dctian@hotmail.com']  \n",
        "#     subject = msg  \n",
        "#     body = '%s\\n\\n- David' % msg\n",
        "\n",
        "#     email_text = \\\n",
        "# \"\"\"From: %s\n",
        "# To: %s\n",
        "# Subject: %s\n",
        "\n",
        "# %s\n",
        "# \"\"\" % (sent_from, \", \".join(to), subject, body)\n",
        "\n",
        "#     server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
        "#     server.ehlo()\n",
        "#     server.starttls()\n",
        "#     server.login(gmail_user, gmail_password)\n",
        "#     server.sendmail(sent_from, to, email_text)\n",
        "#     server.quit()\n",
        "\n",
        "#     print(f'Email: \\n{email_text}')\n",
        "    "
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lvis"
      ],
      "metadata": {
        "id": "SfPUc3YutA4f",
        "outputId": "f90e359d-fa5a-4653-a1ad-947cc9949e63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.8/dist-packages (0.5.3)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (0.11.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis) (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.8/dist-packages (from lvis) (3.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (1.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from lvis) (1.22.4)\n",
            "Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.8/dist-packages (from lvis) (0.29.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDHjhKQofT5",
        "outputId": "e8e124a3-8a56-414e-c8fa-8646f3b7d5fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_steps = 2000\n",
        "#SendEmail(\"Colab train started\")\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir='{model_dir}' \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "#SendEmail(\"Colab train finished\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-22 15:26:56.079148: E tensorflow/core/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/model_main.py\", line 23, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 479, in <module>\n",
            "    keras._load()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\n",
            "    module = importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/__init__.py\", line 25, in <module>\n",
            "    from keras import models\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/models/__init__.py\", line 18, in <module>\n",
            "    from keras.engine.functional import Functional\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 25, in <module>\n",
            "    from keras.engine import base_layer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 43, in <module>\n",
            "    from keras.mixed_precision import loss_scale_optimizer\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\", line 20, in <module>\n",
            "    from keras.optimizer_v2 import optimizer_v2\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 36, in <module>\n",
            "    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 360, in __init__\n",
            "    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py\", line 135, in __init__\n",
            "    self._metric = self._metric_methods[self._label_length].create(*args)\n",
            "tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-tUdtnRybs",
        "outputId": "cb4e3af8-1c25-4e66-8e7e-242ac61c3384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -ltra '{model_dir}'"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwx------ 2 root root 4096 Jan 22 12:16 fine_tuned_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Kzh3_JLVW-"
      },
      "source": [
        "# Section 5: Save and Convert Model Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHoP90pUyKSq"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '%s/fine_tuned_model' % model_dir\n",
        "os.makedirs(output_directory, exist_ok=True)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikck2kvh_wTB",
        "outputId": "90a58b87-00fb-4788-c2da-725c8a1582d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "lst = os.listdir(model_dir)\n",
        "# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-27d50ca0e9ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'model.ckpt-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'.meta'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlast_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.meta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlast_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlxqSTTgHMHO"
      },
      "source": [
        "!echo creates the frozen inference graph in fine_tune_model\n",
        "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --trained_checkpoint_prefix='{last_model_path}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwsBovsWZV_S"
      },
      "source": [
        "# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n",
        "# create the tensorflow lite graph\n",
        "!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --trained_checkpoint_prefix='{last_model_path}' \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --add_postprocessing_op=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTikaJKpK6No"
      },
      "source": [
        "!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n",
        "!tflite_convert \\\n",
        "  --output_file='{output_directory}/road_signs_quantized.tflite' \\\n",
        "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
        "  --inference_type=QUANTIZED_UINT8 \\\n",
        "  --input_arrays='normalized_input_image_tensor' \\\n",
        "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
        "  --mean_values=128 \\\n",
        "  --std_dev_values=128 \\\n",
        "  --input_shapes=1,300,300,3 \\\n",
        "  --change_concat_input_ranges=false \\\n",
        "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
        "  --allow_custom_ops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsZi5SHSSVur"
      },
      "source": [
        "!echo \"CONVERTING frozen graph to unquantized TF Lite file...\"\n",
        "!tflite_convert \\\n",
        "  --output_file='{output_directory}/road_signs_float.tflite' \\\n",
        "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
        "  --input_arrays='normalized_input_image_tensor' \\\n",
        "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
        "  --mean_values=128 \\\n",
        "  --std_dev_values=128 \\\n",
        "  --input_shapes=1,300,300,3 \\\n",
        "  --change_concat_input_ranges=false \\\n",
        "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
        "  --allow_custom_ops \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usgBZvkz0nqD"
      },
      "source": [
        "print(output_directory)\n",
        "!ls -ltra '{output_directory}'\n",
        "#pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\") # this is main one\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")  # this is tflite graph\n",
        "!cp '{label_map_pbtxt_fname}' '{output_directory}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1gX19GlVW7"
      },
      "source": [
        "## Run inference test\n",
        "Test with images in repository `object_detection/data/images/test` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzj9A4e5mj5l"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "print(PATH_TO_CKPT)\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"models/object_detection/data/images/test\")\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5YUMdg1Po7"
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEmFi1DFGBQ8"
      },
      "source": [
        "# running inferences.  This should show images with bounding boxes\n",
        "%matplotlib inline\n",
        "\n",
        "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=2)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBbsZRWt0h6l"
      },
      "source": [
        "## Convert to Edge TPU's tflite Format  \n",
        "The only known way, at time of writing (April 2019), is to download the below quantized tflite file from above, and use [Google's web compiler](https://coral.withgoogle.com/web-compiler/) to convert to Edge TPU's tflite format.   Unfortunately, this step has to be done by hand, and NOT via a script.  \n",
        "\n",
        "Here are the requirements of Edge TPU web compiler.  If you have followed the above steps closely, you have met these requirements.\n",
        "\n",
        "- Tensor parameters are quantized (8-bit fixed-point numbers). You must use quantization-aware training (post-training quantization is not supported).   (this is why we are using `ssd_mobilenet_v2_quantized` base model and not the  `ssd_mobilenet_v2` base model   \n",
        "- Tensor sizes are constant at compile-time (no dynamic sizes).\n",
        "- Model parameters (such as bias tensors) are constant at compile-time.\n",
        "- Tensors are either 1-, 2-, or 3-dimensional. If a tensor has more than 3 dimensions, then only the 3 innermost dimensions may have a size greater than 1.\n",
        "- The model uses only the operations supported by the Edge TPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HhQ9y_Jcrfa"
      },
      "source": [
        "# download this file from google drive.\n",
        "!ls -lt '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/road_signs_quantized.tflite'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WzRVnDj0jt9"
      },
      "source": [
        "Wait for about 1-2 minutes for compilation to finish.  And we can download the model file as `road_signs_quantized_edgetpu.tflite`.  This is the file you need to copy to raspberry pi with TPU to run object detection.\n",
        "\n",
        "We are all done with colab notebook training, now time to switch back to raspberry pi, and run `~/DeepPiCar/models/object_detection/code/object_detection_usb.py`.  You should see a video feed where road sign and persons are boxed with confidence level around them.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG852pD0Buaq"
      },
      "source": [
        "```bash\n",
        "# make sure the the road_signs_quantized_edgetpu.tflite is in the right directory in your pi\n",
        "pi@raspberrypi:~/DeepPiCar/models/object_detection/data/model_result $ ls -ltr\n",
        "total 10040\n",
        "-rw-r--r-- 1 pi pi      97 Apr 15 01:01 road_sign_labels.txt\n",
        "-rw-r--r-- 1 pi pi 4793504 Apr 16 15:49 road_signs_quantized.tflite\n",
        "-rw-r--r-- 1 pi pi 5478080 Apr 16 15:49 road_signs_quantized_edgetpu.tflite\n",
        "\n",
        "pi@raspberrypi:~/DeepPiCar/models/object_detection $ python3 code/object_detection_usb.py\n",
        "\n",
        "------\n",
        "2019-04-16 16:22:28.489224: 13.49 FPS, 74.12ms total, 70.84ms in tf \n",
        "Green Traffic Light, 80% [[240.61578751 131.68985367]\n",
        " [287.21975327 195.79172134]] 60.42ms\n",
        "Stop Sign, 44% [[  0.         305.1651001 ]\n",
        " [180.84949493 409.32563782]] 60.42ms\n",
        "\n",
        "------\n",
        "2019-04-16 16:22:28.618309: 14.83 FPS, 67.44ms total, 60.42ms in tf \n",
        "Person, 89% [[505.6583786  279.52325821]\n",
        " [530.85933685 360.0169754 ]] 62.54ms\n",
        "Green Traffic Light, 72% [[237.96649933 130.58757782]\n",
        " [283.52127075 203.24180603]] 62.54ms\n",
        "Red Traffic Light, 62% [[283.23583603 169.27398682]\n",
        " [330.91316223 269.20692444]] 62.54ms\n",
        "Stop Sign, 56% [[ 51.01628304 165.80377579]\n",
        " [101.48646355 227.05183029]] 62.54ms\n",
        "Person, 44% [[396.8661499  298.65327835]\n",
        " [468.4034729  422.04421997]] 62.54ms\n",
        "------\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqSFdHQvElks"
      },
      "source": [
        "# Section 6: Last Words on this Project\n",
        "\n",
        "Of course, depending many factors, not all objects in the video frame will be identified.  This is the chance to improve your model.   Try to train longer, or train with more labeled images, or augment your existing images with different zooms/rotations/contrast/lighting.  In my case, I started with one camera, which was somewhat fuzzy, and precision low.  When I switched to a HD camera, the model precision was significantly better.   This was just another way.\n",
        "\n",
        "It is awesome that for just over $100 in hardware, we can do real time object detection at home.   Moreover, thanks to Google, all you need is a browser to train this huge model!  Having fun with your own raspberry pi object detection projects!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eswamFiJLrnM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}