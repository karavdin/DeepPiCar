{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_traffic_sign_detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karavdin/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQCnYPVDrsgx"
      },
      "source": [
        "# Training a Raspberry Pi to Detect Traffic Signs and People in Real Time\n",
        "\n",
        "This is tutorial is based on Chengwei's excellent Tutorial and Colab Notebook on [\"How to train an object detection model easy for free\"](https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/).   My twist on his tutorial is that I need to run my model on a Raspberry Pi with live video feed.  As the Raspberry Pi is fairly limited on CPU power and can only run object detection at 1-2 FPS (frames/sec), I have purchased the newly release $75 Google's [EdgeTPU USB Accelarator](https://coral.withgoogle.com/products/accelerator), which can detect objects at 12 FPS, which is sufficient for real time work.  After doing the transfer learning from one of the object detection models using our own images, last few steps of the colab deals with how to convert a trained model to a model file that can be consumed by an Edge TPU, namely, the final `mymodel_quantized_edgetpu.tflite` file.  \n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*_jABdMfUVcyPdi5b3zlfVg.jpeg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOThFKokSmU"
      },
      "source": [
        "# Section 1: Mount Google drive\n",
        "Mount my Google Drive and save modeling output files (`.ckpt`)  there, so that it won't be wiped out when colab Virtual Machine restarts.  It has an idle timeout of 90 min, and maximum daily usage of 12 hours.\n",
        "\n",
        "Google will ask for an authenticate code when you run the following code, just follow the link in the output and allow access.   You can put the `model_dir` anywhere in your google drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DEe8d3SAZRvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA1y7NMxFT_B",
        "outputId": "e095b813-40e2-4cfe-cd2f-d2ef6d197474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "model_dir = '/content/gdrive/My Drive/Colab Notebooks/DeepCar_Training'\n",
        "#!rm -rf '{model_dir}'\n",
        "#os.makedirs(model_dir, exist_ok=True)\n",
        "!ls -ltra '{model_dir}'/.."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "total 27181\n",
            "-rw------- 1 root root   79210 Jun 30  2019  my_em_assignment.ipynb\n",
            "-rw------- 1 root root  395675 Jul  9  2019 'my mcmc_assignment.ipynb'\n",
            "-rw------- 1 root root  206381 Jul 24  2019  my_Vae_assignment.ipynb\n",
            "-rw------- 1 root root  243880 Jul 28  2019  my_gp_assignment.ipynb\n",
            "-rw------- 1 root root  968824 Jul 29  2019  my_finding_suspect.ipynb\n",
            "-rw------- 1 root root     287 Aug 22  2019  Untitled0.ipynb\n",
            "-rw------- 1 root root 1698449 Aug 22  2019 'Bike Sharing Prediction.ipynb'\n",
            "-rw------- 1 root root     287 Aug 27  2019  Untitled1.ipynb\n",
            "-rw------- 1 root root  781572 Aug 28  2019 'Trip waiting time prediction.ipynb'\n",
            "-rw------- 1 root root   19520 Sep 10  2019 'Data Engineer Challenge Swarovski.ipynb'\n",
            "-rw------- 1 root root     269 Sep 22  2019  Untitled2.ipynb\n",
            "-rw------- 1 root root  190302 Sep 24  2019  Prototype2.ipynb\n",
            "-rw------- 1 root root  128216 Oct 30  2019 'My Copy of Exercise_Generative_Models_Problem.ipynb'\n",
            "-rw------- 1 root root  268926 Oct 30  2019 'My Copy of Exercise_Generative_Models_Solution.ipynb'\n",
            "-rw------- 1 root root   42798 Nov 10  2019 'Copy of Course 1 - Part 4 - Lesson 2 - Notebook.ipynb'\n",
            "-rw------- 1 root root   39200 Nov 10  2019 'Copy of Course 1 - Part 6 - Lesson 2 - Notebook.ipynb'\n",
            "-rw------- 1 root root 2045038 Nov 10  2019 'Copy of Horse-or-Human-NoValidation.ipynb'\n",
            "-rw------- 1 root root 1365097 Nov 11  2019 'Copy of Course 2 - Part 4 - Lesson 2 - Notebook.ipynb'\n",
            "-rw------- 1 root root   33938 Nov 16  2019 'Copy of Transfer Learning.ipynb'\n",
            "-rw------- 1 root root   13584 Nov 24  2019 'Copy of S+P Week 2 Lesson 1 (preparing features and lables for time series).ipynb'\n",
            "-rw------- 1 root root  167061 Nov 25  2019 'Copy of S+P Week 3 Lesson 4 - LSTM.ipynb'\n",
            "-rw------- 1 root root   36836 Jun 20  2020  Etsy_Geometry_of_Time_Stats_2020.ipynb\n",
            "-rw------- 1 root root   36917 Jun 20  2020 'Copy of Etsy_Geometry_of_Time_Stats_2020.ipynb'\n",
            "-rw------- 1 root root    4388 Aug 15  2021  Untitled3.ipynb\n",
            "-rw------- 1 root root     410 Dec 24  2021  Yandex_Praktikum_DS_DA.ipynb\n",
            "-rw------- 1 root root   14119 Dec 24  2021  Task1.ipynb\n",
            "-rw------- 1 root root 6591516 Dec 28  2021  shap-implementation-1d2cca5e12634c4f96fb56e6dcee44f1.ipynb\n",
            "-rw------- 1 root root 3060145 Dec 28  2021 'My Version Explainable AI Graded Hands-on Exercise.ipynb'\n",
            "-rw------- 1 root root 1917384 Jan  5  2022  etsy_krabbeldecke.ipynb\n",
            "-rw------- 1 root root 3384812 Jan  5  2022  etsy_countries.ipynb\n",
            "-rw------- 1 root root 3947369 Jan  6  2022  etsy_listings.ipynb\n",
            "-rw------- 1 root root   80815 Nov 19 12:40 \"Anastasia's Copy of Tweet Emotion Recognition - Learner.ipynb\"\n",
            "drwx------ 2 root root    4096 Jan 22 11:00  DeepCar_Training\n",
            "-rw------- 1 root root   57225 Jan 22 11:01 'Kopie von tensorflow_traffic_sign_detection.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzxsJb3dpWq"
      },
      "source": [
        "# Section 2: Configs and Hyperparameters\n",
        "\n",
        "Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNXNQCjdniL"
      },
      "source": [
        "# If you forked the repository, you can replace the link.\n",
        "repo_url = 'https://github.com/karavdin/DeepPiCar'\n",
        "\n",
        "# Number of training steps.\n",
        "#num_steps = 1000  # 200000\n",
        "num_steps = 100  # 200000\n",
        "\n",
        "# Number of evaluation steps.\n",
        "#num_eval_steps = 50\n",
        "num_eval_steps = 5\n",
        "\n",
        "\n",
        "# model configs are from Model Zoo github: \n",
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
        "MODELS_CONFIG = {\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n",
        "    'ssd_mobilenet_v1_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n",
        "        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
        "        'batch_size': 12\n",
        "    },    \n",
        "    # 'ssd_mobilenet_v2': {\n",
        "    #     'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "    #     'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "    #     'batch_size': 12\n",
        "    # },\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "    'ssd_mobilenet_v2_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    #http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "    'ssd_mobilenet_v2_tf2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        #'pipeline_file': 'ssd_mobilenet_v3_small.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    \n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "# Note: for Edge TPU, you have to:\n",
        "# 1) start with a pretrained model from model zoo, such as above 4\n",
        "# 2) Must be a quantized model, which reduces the model size significantly\n",
        "#selected_model = 'ssd_mobilenet_v2_quantized'\n",
        "selected_model = 'ssd_mobilenet_v2_tf2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "#pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-YqZHUKv-Y"
      },
      "source": [
        "# Section 3: Set up Training Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_xVkoa4KjHO"
      },
      "source": [
        "## Clone the `DeepPiCar` repository or your fork."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxc3DmvLQF3z",
        "outputId": "00e55d36-6c3e-49f2-90e8-f29e4d08fb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "print('Pull it so that we have the latest code/data')\n",
        "!git pull"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DeepPiCar'...\n",
            "remote: Enumerating objects: 1334, done.\u001b[K\n",
            "remote: Counting objects: 100% (307/307), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 1334 (delta 158), reused 206 (delta 109), pack-reused 1027\u001b[K\n",
            "Receiving objects: 100% (1334/1334), 184.37 MiB | 14.55 MiB/s, done.\n",
            "Resolving deltas: 100% (479/479), done.\n",
            "/content/DeepPiCar\n",
            "Pull it so that we have the latest code/data\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8__uNS8-ns"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "sBTP_y94F0hH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "# !pip install tf_slim\n",
        "# !pip install tensorflow_io\n",
        "# !pip install tensorflow-addons\n",
        "!pip install tensorflow==2.11.0\n",
        "#!pip install --ignore-installed --upgrade tensorflow==1.15"
      ],
      "metadata": {
        "id": "KGiQGSg5ByEK",
        "outputId": "0044d994-0c07-4240-f960-d3d416177ab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.3.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (15.0.6.1)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (21.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.29.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (4.4.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.11.0) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-23.1.21 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"--- Tensorflow version --- \\n\", tf.__version__)\n",
        "#print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
      ],
      "metadata": {
        "id": "OlUNke4GDMYI",
        "outputId": "1c87e37d-8b85-46ee-a6c3-a68312cc6e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tensorflow version --- \n",
            " 2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecpHEnka8Kix",
        "outputId": "6307494f-67ba-4278-bdc5-54e8e7a9ebfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q cython contextlib2 pillow lxml matplotlib\n",
        "#!pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
        "\n",
        "\n",
        "# !git clone https://github.com/cocodataset/cocoapi.git\n",
        "# %cd cocoapi/PythonAPI\n",
        "# !make\n",
        "# !cp -r pycocotools /content/models/research/\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install --use-feature=2020-resolver .\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/:/content/models/'\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/models\")\n",
        "\n",
        "#!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 129499 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 129528 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "Preparing to unpack .../01-libimagequant0_2.12.2-1.1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.12.2-1.1) ...\n",
            "Selecting previously unselected package python-backports.functools-lru-cache.\n",
            "Preparing to unpack .../02-python-backports.functools-lru-cache_1.5-3build1_all.deb ...\n",
            "Unpacking python-backports.functools-lru-cache (1.5-3build1) ...\n",
            "Selecting previously unselected package python-soupsieve.\n",
            "Preparing to unpack .../03-python-soupsieve_1.9.5+dfsg-1_all.deb ...\n",
            "Unpacking python-soupsieve (1.9.5+dfsg-1) ...\n",
            "Selecting previously unselected package python-bs4.\n",
            "Preparing to unpack .../04-python-bs4_4.8.2-1_all.deb ...\n",
            "Unpacking python-bs4 (4.8.2-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../05-python-pkg-resources_44.0.0-2ubuntu0.1_all.deb ...\n",
            "Unpacking python-pkg-resources (44.0.0-2ubuntu0.1) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../06-python-chardet_3.0.4-4build1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-4build1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../07-python-six_1.14.0-2_all.deb ...\n",
            "Unpacking python-six (1.14.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../08-python-webencodings_0.5.1-1ubuntu1_all.deb ...\n",
            "Unpacking python-webencodings (0.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../09-python-html5lib_1.0.1-2_all.deb ...\n",
            "Unpacking python-html5lib (1.0.1-2) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../10-python-lxml_4.5.0-1ubuntu0.5_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.5.0-1ubuntu0.5) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../11-python-olefile_0.46-2_all.deb ...\n",
            "Unpacking python-olefile (0.46-2) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../12-python-pil_6.2.1-3_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (6.2.1-3) ...\n",
            "Selecting previously unselected package python-tk.\n",
            "Preparing to unpack .../13-python-tk_2.7.18-1_amd64.deb ...\n",
            "Unpacking python-tk (2.7.18-1) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up python-six (1.14.0-2) ...\n",
            "Setting up libimagequant0:amd64 (2.12.2-1.1) ...\n",
            "Setting up python-backports.functools-lru-cache (1.5-3build1) ...\n",
            "Setting up python-tk (2.7.18-1) ...\n",
            "Setting up python-webencodings (0.5.1-1ubuntu1) ...\n",
            "Setting up python-olefile (0.46-2) ...\n",
            "Setting up python-lxml:amd64 (4.5.0-1ubuntu0.5) ...\n",
            "Setting up python-html5lib (1.0.1-2) ...\n",
            "Setting up python-pkg-resources (44.0.0-2ubuntu0.1) ...\n",
            "Setting up python-pil:amd64 (6.2.1-3) ...\n",
            "Setting up python-soupsieve (1.9.5+dfsg-1) ...\n",
            "Setting up python-chardet (3.0.4-4build1) ...\n",
            "Setting up python-bs4 (4.8.2-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "/content/models/research\n",
            "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (4.9.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.29.33)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.0.6)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.11.3-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.30.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.11.0)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu<=2.2.0\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Collecting immutabledict\n",
            "  Downloading immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 KB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.70.0)\n",
            "Requirement already satisfied: tensorflow~=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.7.0.68)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.8.1)\n",
            "Collecting tensorflow-text~=2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2022.7)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from tf-slim->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.22.2)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.25.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.5-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.2/526.2 KB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (4.4.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.51.1)\n",
            "Collecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle~=2.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.2.0)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Collecting objsize<0.7.0,>=0.6.1\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.30.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.1.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.12.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (23.1.21)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.11.2)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.10.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.58.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (6.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection, avro-python3, dill, seqeval, docopt\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=21919980 sha256=a30b980ae4a76663ddafdcfaa26c65075a4618a5576c96aa1e98c5c1cc1ba2cc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gb5he4tg/wheels/7d/96/c1/072a751379735e8dfdada1def1c62a89afb3cc45654fd6fd28\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44009 sha256=caa22ac27392c9eca7a2231f8e8c154c1688010000487661b87f35bd8bb436cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/73/e9/d273421f5723c4bf544dcf9eb097bda94421ef8d3252699f0a\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=d274ed99ca4f81be1d74a7f934ed05988d279c5f78611acf68f94042f3cdaae3\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=720b9a1b2a26e65bb4f53961808039ab664b981daef540efa848efe686643a9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=688bd8bc3cd7f9cff83b63fe8fa42f0c0554763d33e20777f8ba3f713d299c50\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built object-detection avro-python3 dill seqeval docopt\n",
            "Installing collected packages: sentencepiece, py-cpuinfo, docopt, zstandard, tf-slim, tensorflow-model-optimization, tensorflow-io-gcs-filesystem, pyyaml, pyparsing, pymongo, portalocker, orjson, objsize, immutabledict, fasteners, fastavro, dill, colorama, avro-python3, tensorflow_io, sacrebleu, hdfs, tensorflow-addons, seqeval, lvis, apache-beam, tensorflow-text, tf-models-official, object-detection\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.29.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.29.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.29.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.3\n",
            "    Uninstalling pymongo-4.3.3:\n",
            "      Successfully uninstalled pymongo-4.3.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed apache-beam-2.44.0 avro-python3-1.10.2 colorama-0.4.6 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.1 fasteners-0.18 hdfs-2.7.0 immutabledict-2.2.3 lvis-0.5.3 object-detection-0.1 objsize-0.6.1 orjson-3.8.5 portalocker-2.7.0 py-cpuinfo-9.0.0 pymongo-3.13.0 pyparsing-2.4.7 pyyaml-5.4.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorflow-addons-0.19.0 tensorflow-io-gcs-filesystem-0.30.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 tensorflow_io-0.30.0 tf-models-official-2.11.3 tf-slim-1.1.0 zstandard-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "metadata": {
        "id": "DW8sd8Y_B3W7",
        "outputId": "827c9ff5-c53c-4119-b237-333a848d2fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-28 14:46:45.790171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:46:45.790342: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:46:45.790371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-01-28 14:46:50.966980: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Running tests under Python 3.8.10: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "W0128 14:46:51.818176 140671366220864 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.65s\n",
            "I0128 14:46:52.626093 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.65s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.84s\n",
            "I0128 14:46:54.469055 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.84s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 1.29s\n",
            "I0128 14:46:55.761185 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 1.29s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.72s\n",
            "I0128 14:46:56.493993 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.72s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 12.02s\n",
            "I0128 14:47:08.512480 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 12.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0128 14:47:08.521650 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.1s\n",
            "I0128 14:47:08.626812 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.1s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.96s\n",
            "I0128 14:47:09.596132 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.96s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.14s\n",
            "I0128 14:47:09.743828 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.51s\n",
            "I0128 14:47:10.252330 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.51s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.43s\n",
            "I0128 14:47:10.681101 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.43s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.6s\n",
            "I0128 14:47:11.282346 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.6s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.53s\n",
            "I0128 14:47:11.818686 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.53s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.3s\n",
            "I0128 14:47:12.118482 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.3s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.06s\n",
            "I0128 14:47:12.180177 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.06s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0128 14:47:13.124158 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0128 14:47:13.124407 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 64\n",
            "I0128 14:47:13.124500 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 3\n",
            "I0128 14:47:13.129635 140671366220864 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0128 14:47:13.292013 140671366220864 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0128 14:47:13.297688 140671366220864 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0128 14:47:13.531032 140671366220864 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0128 14:47:13.537938 140671366220864 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0128 14:47:14.131976 140671366220864 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0128 14:47:14.132298 140671366220864 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0128 14:47:14.747522 140671366220864 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0128 14:47:14.747940 140671366220864 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0128 14:47:15.822581 140671366220864 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0128 14:47:15.829990 140671366220864 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0128 14:47:16.821612 140671366220864 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0128 14:47:16.821966 140671366220864 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0128 14:47:18.153781 140671366220864 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0128 14:47:18.154092 140671366220864 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0128 14:47:18.301425 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0128 14:47:18.397726 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:18.479513 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0128 14:47:18.479831 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n",
            "I0128 14:47:18.479953 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n",
            "I0128 14:47:18.482585 140671366220864 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0128 14:47:18.510155 140671366220864 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0128 14:47:18.510422 140671366220864 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0128 14:47:18.761159 140671366220864 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0128 14:47:18.761413 140671366220864 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0128 14:47:19.150283 140671366220864 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0128 14:47:19.150531 140671366220864 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0128 14:47:19.550729 140671366220864 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0128 14:47:19.551021 140671366220864 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0128 14:47:20.117514 140671366220864 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0128 14:47:20.117801 140671366220864 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0128 14:47:20.591857 140671366220864 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0128 14:47:20.592158 140671366220864 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0128 14:47:21.256051 140671366220864 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0128 14:47:21.256312 140671366220864 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0128 14:47:21.587262 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0128 14:47:21.675647 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:22.068055 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0128 14:47:22.068324 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 112\n",
            "I0128 14:47:22.068417 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 5\n",
            "I0128 14:47:22.070771 140671366220864 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0128 14:47:22.097961 140671366220864 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0128 14:47:22.098238 140671366220864 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0128 14:47:22.294180 140671366220864 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0128 14:47:22.294440 140671366220864 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0128 14:47:22.718870 140671366220864 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0128 14:47:22.719156 140671366220864 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0128 14:47:23.144438 140671366220864 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0128 14:47:23.144744 140671366220864 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0128 14:47:23.711068 140671366220864 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0128 14:47:23.711336 140671366220864 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0128 14:47:24.323630 140671366220864 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0128 14:47:24.323914 140671366220864 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0128 14:47:25.111700 140671366220864 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0128 14:47:25.111968 140671366220864 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I0128 14:47:25.446254 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I0128 14:47:25.530415 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:25.611098 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0128 14:47:25.611311 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 160\n",
            "I0128 14:47:25.611401 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 6\n",
            "I0128 14:47:25.613473 140671366220864 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0128 14:47:25.637143 140671366220864 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0128 14:47:25.637340 140671366220864 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0128 14:47:25.845552 140671366220864 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0128 14:47:25.845767 140671366220864 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0128 14:47:26.214204 140671366220864 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0128 14:47:26.214404 140671366220864 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0128 14:47:26.542583 140671366220864 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0128 14:47:26.542788 140671366220864 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0128 14:47:27.138315 140671366220864 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0128 14:47:27.138521 140671366220864 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0128 14:47:27.828031 140671366220864 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0128 14:47:27.828320 140671366220864 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0128 14:47:28.802913 140671366220864 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0128 14:47:28.803241 140671366220864 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I0128 14:47:29.179687 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I0128 14:47:29.269831 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:29.360677 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0128 14:47:29.360964 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 224\n",
            "I0128 14:47:29.361057 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I0128 14:47:29.363372 140671366220864 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0128 14:47:29.392577 140671366220864 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0128 14:47:29.392832 140671366220864 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0128 14:47:29.575045 140671366220864 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0128 14:47:29.575283 140671366220864 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0128 14:47:30.031288 140671366220864 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0128 14:47:30.031500 140671366220864 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0128 14:47:30.496145 140671366220864 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0128 14:47:30.496331 140671366220864 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0128 14:47:31.185941 140671366220864 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0128 14:47:31.186149 140671366220864 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0128 14:47:32.277957 140671366220864 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0128 14:47:32.278175 140671366220864 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0128 14:47:33.677726 140671366220864 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0128 14:47:33.677959 140671366220864 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I0128 14:47:34.032277 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I0128 14:47:34.117279 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:34.206950 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0128 14:47:34.207154 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 288\n",
            "I0128 14:47:34.207243 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I0128 14:47:34.209309 140671366220864 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0128 14:47:34.231105 140671366220864 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0128 14:47:34.231291 140671366220864 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0128 14:47:34.501164 140671366220864 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0128 14:47:34.501362 140671366220864 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0128 14:47:35.184942 140671366220864 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0128 14:47:35.185189 140671366220864 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0128 14:47:35.845949 140671366220864 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0128 14:47:35.846191 140671366220864 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0128 14:47:36.740289 140671366220864 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0128 14:47:36.740559 140671366220864 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0128 14:47:37.802061 140671366220864 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0128 14:47:37.802329 140671366220864 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0128 14:47:39.377125 140671366220864 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0128 14:47:39.377380 140671366220864 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I0128 14:47:40.004839 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I0128 14:47:40.101068 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:40.210638 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0128 14:47:40.210860 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n",
            "I0128 14:47:40.210949 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I0128 14:47:40.213063 140671366220864 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0128 14:47:40.239668 140671366220864 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0128 14:47:40.239878 140671366220864 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0128 14:47:40.525032 140671366220864 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0128 14:47:40.525240 140671366220864 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0128 14:47:41.204725 140671366220864 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0128 14:47:41.204960 140671366220864 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0128 14:47:41.915443 140671366220864 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0128 14:47:41.915720 140671366220864 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0128 14:47:43.399396 140671366220864 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0128 14:47:43.399664 140671366220864 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0128 14:47:44.446164 140671366220864 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0128 14:47:44.446388 140671366220864 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0128 14:47:46.128275 140671366220864 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0128 14:47:46.128486 140671366220864 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I0128 14:47:46.759076 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I0128 14:47:46.858924 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0128 14:47:46.973616 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0128 14:47:46.973866 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n",
            "I0128 14:47:46.973966 140671366220864 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I0128 14:47:46.975963 140671366220864 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0128 14:47:46.998270 140671366220864 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0128 14:47:46.998461 140671366220864 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0128 14:47:47.360876 140671366220864 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0128 14:47:47.361073 140671366220864 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0128 14:47:48.173679 140671366220864 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0128 14:47:48.173968 140671366220864 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0128 14:47:49.132137 140671366220864 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0128 14:47:49.132388 140671366220864 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0128 14:47:50.514280 140671366220864 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0128 14:47:50.514539 140671366220864 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0128 14:47:51.859680 140671366220864 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0128 14:47:51.859893 140671366220864 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0128 14:47:54.575428 140671366220864 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0128 14:47:54.575719 140671366220864 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I0128 14:47:55.570188 140671366220864 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I0128 14:47:55.693198 140671366220864 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 43.65s\n",
            "I0128 14:47:55.835386 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 43.65s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0128 14:47:55.871059 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0128 14:47:55.873180 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0128 14:47:55.873792 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0128 14:47:55.875435 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0128 14:47:55.876849 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0128 14:47:55.877263 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0128 14:47:55.878306 140671366220864 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 64.907s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny"
      },
      "source": [
        "## Prepare `tfrecord` files\n",
        "\n",
        "Use the following scripts to generate the `tfrecord` files.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezGDABRXXhPP",
        "outputId": "ddc82499-69f3-41c3-8c9f-52543a1082ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd {repo_dir_path}/models/object_detection\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python code/xml_to_csv.py -i data/images_hh/train -o data/annotations_hh/train_labels.csv -l data/annotations_hh\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python code/xml_to_csv.py -i data/images_hh/test -o data/annotations_hh/test_labels.csv\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepPiCar/models/object_detection\n",
            "Successfully converted xml to csv.\n",
            "Generate `data/annotations_hh/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "u5yq_Xv7VOhy",
        "outputId": "5e5e28e5-bc7d-4431-fe46-7b78b0041ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code  data  docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_labels = pd.read_csv('data/annotations_hh/train_labels.csv')\n",
        "print(df_train_labels['class'].unique())"
      ],
      "metadata": {
        "id": "dtue93YVP3vP",
        "outputId": "ec3c147d-3513-49d5-bc4c-d2d43d3edab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Capitan' 'Nastya' 'Cam' 'Sheila' 'Bob' 'bike' 'Babushka'\n",
            " 'e-bike station' 'Phil' 'clock' 'ukulele' 'disko' 'Ukulele']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_labels = pd.read_csv('data/annotations_hh/test_labels.csv')\n",
        "print(df_test_labels['class'].unique())"
      ],
      "metadata": {
        "id": "-MLvAaiWRo1-",
        "outputId": "3fcf1a1c-7b0c-4f4f-9368-3532bc280680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['clock' 'bike' 'Babushka' 'Nastya' 'e-bike station' 'Sheila' 'Capitan'\n",
            " 'Bob' 'Phil' 'Cam' 'ukulele']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate `train.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations_hh/train_labels.csv --output_path=data/annotations_hh/train.record --img_path=data/images_hh/train --label_map data/annotations_hh/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations_hh/test_labels.csv --output_path=data/annotations_hh/test.record --img_path=data/images_hh/test --label_map data/annotations_hh/label_map.pbtxt"
      ],
      "metadata": {
        "id": "JiWqYfQ7P6S8",
        "outputId": "f18299be-b018-4333-a6e3-c10edaa67f9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-28 14:48:01.884101: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:48:01.884305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:48:01.884333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-01-28 14:48:05.072906: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Successfully created the TFRecords: /content/DeepPiCar/models/object_detection/data/annotations_hh/train.record\n",
            "2023-01-28 14:48:11.029257: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:48:11.029441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:48:11.029469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-01-28 14:48:14.309437: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Successfully created the TFRecords: /content/DeepPiCar/models/object_detection/data/annotations_hh/test.record\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgd-fzAIkZlV"
      },
      "source": [
        "test_record_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/test.record'\n",
        "train_record_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/train.record'\n",
        "label_map_pbtxt_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/label_map.pbtxt'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8otnfyCYdG",
        "outputId": "ea9a3c06-467b-464d-eede-3e8e907eb9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat data/annotations_hh/test_labels.csv"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filename,width,height,class,xmin,ymin,xmax,ymax\n",
            "test_37.jpg,2592,1944,clock,10,454,304,1114\n",
            "test_37.jpg,2592,1944,bike,121,948,299,1270\n",
            "test_37.jpg,2592,1944,Babushka,675,954,824,1250\n",
            "test_37.jpg,2592,1944,Nastya,812,974,870,1123\n",
            "test_37.jpg,2592,1944,e-bike station,1264,663,1639,1290\n",
            "test_37.jpg,2592,1944,Sheila,1712,1028,1786,1197\n",
            "test_37.jpg,2592,1944,Capitan,1924,970,2221,1514\n",
            "test_37.jpg,2592,1944,Bob,2272,1034,2448,1341\n",
            "test_37.jpg,2592,1944,Phil,2019,746,2239,994\n",
            "test_2.jpg,2592,1944,Bob,2357,1019,2592,1399\n",
            "test_2.jpg,2592,1944,Phil,2099,694,2348,1219\n",
            "test_2.jpg,2592,1944,Capitan,1908,1012,2041,1237\n",
            "test_2.jpg,2592,1944,clock,1804,803,2044,1150\n",
            "test_2.jpg,2592,1944,Babushka,1495,992,1637,1146\n",
            "test_2.jpg,2592,1944,Cam,1312,994,1404,1112\n",
            "test_2.jpg,2592,1944,Sheila,1052,963,1161,1123\n",
            "test_2.jpg,2592,1944,Nastya,670,959,768,1159\n",
            "test_2.jpg,2592,1944,ukulele,695,1026,752,1128\n",
            "test_2.jpg,2592,1944,e-bike station,237,452,666,1294\n",
            "test_28.jpg,2592,1944,Nastya,715,952,837,1197\n",
            "test_28.jpg,2592,1944,ukulele,761,1001,808,1172\n",
            "test_28.jpg,2592,1944,Capitan,935,943,1090,1286\n",
            "test_28.jpg,2592,1944,Cam,1235,992,1346,1172\n",
            "test_28.jpg,2592,1944,bike,1126,1003,1199,1121\n",
            "test_28.jpg,2592,1944,Sheila,1555,1012,1650,1194\n",
            "test_28.jpg,2592,1944,Bob,1686,994,1941,1481\n",
            "test_28.jpg,2592,1944,bike,1921,1070,2097,1281\n",
            "test_28.jpg,2592,1944,Babushka,2275,1019,2530,1459\n",
            "test_28.jpg,2592,1944,Phil,2008,746,2244,1083\n",
            "test_28.jpg,2592,1944,e-bike station,1628,868,1661,1008\n",
            "test_6.jpg,2592,1944,Nastya,1161,984,1227,1091\n",
            "test_6.jpg,2592,1944,ukulele,1198,1006,1222,1082\n",
            "test_6.jpg,2592,1944,bike,1333,1046,1478,1270\n",
            "test_6.jpg,2592,1944,Sheila,1696,970,1904,1376\n",
            "test_6.jpg,2592,1944,Capitan,1919,1005,2042,1226\n",
            "test_6.jpg,2592,1944,Phil,2094,722,2361,1223\n",
            "test_6.jpg,2592,1944,Bob,2370,1035,2586,1378\n",
            "test_6.jpg,2592,1944,clock,1779,812,2048,1137\n",
            "test_22.jpg,2592,1944,bike,419,983,615,1194\n",
            "test_22.jpg,2592,1944,Babushka,881,963,1070,1286\n",
            "test_22.jpg,2592,1944,clock,946,788,1175,972\n",
            "test_22.jpg,2592,1944,Nastya,1328,992,1415,1148\n",
            "test_22.jpg,2592,1944,Cam,1439,977,1624,1337\n",
            "test_22.jpg,2592,1944,Capitan,1668,999,1777,1212\n",
            "test_22.jpg,2592,1944,Sheila,1937,1037,2146,1512\n",
            "test_22.jpg,2592,1944,Phil,1895,706,2126,1034\n",
            "test_23.jpg,2592,1944,bike,261,988,675,1568\n",
            "test_23.jpg,2592,1944,Babushka,681,923,830,1237\n",
            "test_23.jpg,2592,1944,Sheila,1375,999,1579,1323\n",
            "test_23.jpg,2592,1944,Capitan,1650,994,1784,1212\n",
            "test_23.jpg,2592,1944,Sheila,1944,1032,2144,1532\n",
            "test_23.jpg,2592,1944,clock,941,781,1175,1092\n",
            "test_23.jpg,2592,1944,Nastya,1344,990,1404,1132\n",
            "test_23.jpg,2592,1944,e-bike station,1684,861,1730,999\n",
            "test_23.jpg,2592,1944,Phil,1899,697,2106,1023\n",
            "test_10.jpg,2592,1944,Nastya,557,948,752,1286\n",
            "test_10.jpg,2592,1944,ukulele,650,992,730,1239\n",
            "test_10.jpg,2592,1944,Capitan,1044,959,1148,1108\n",
            "test_10.jpg,2592,1944,Babushka,1359,994,1441,1137\n",
            "test_10.jpg,2592,1944,Phil,641,653,911,1113\n",
            "test_10.jpg,2592,1944,Sheila,1707,1008,1796,1200\n",
            "test_10.jpg,2592,1944,bike,1832,1044,1900,1160\n",
            "test_10.jpg,2592,1944,e-bike station,1565,845,1723,1125\n",
            "test_10.jpg,2592,1944,clock,1797,809,2032,1137\n",
            "test_10.jpg,2592,1944,Bob,2332,1028,2589,1393\n",
            "test_44.jpg,2592,1944,Bob,337,926,468,1199\n",
            "test_44.jpg,2592,1944,Babushka,721,934,972,1266\n",
            "test_44.jpg,2592,1944,Phil,1097,746,1297,1119\n",
            "test_44.jpg,2592,1944,Cam,1359,981,1535,1243\n",
            "test_44.jpg,2592,1944,Sheila,1706,1023,1786,1181\n",
            "test_44.jpg,2592,1944,Capitan,1892,974,2232,1501\n",
            "test_44.jpg,2592,1944,bike,2404,1070,2592,1488\n",
            "test_47.jpg,2592,1944,Bob,326,921,459,1201\n",
            "test_47.jpg,2592,1944,Babushka,737,941,944,1270\n",
            "test_47.jpg,2592,1944,bike,939,986,997,1103\n",
            "test_47.jpg,2592,1944,Phil,1101,743,1295,1021\n",
            "test_47.jpg,2592,1944,e-bike station,1106,323,1895,1588\n",
            "test_47.jpg,2592,1944,bike,2166,1059,2335,1361\n",
            "test_47.jpg,2592,1944,clock,2012,763,2270,1190\n",
            "test_41.jpg,2592,1944,bike,128,948,290,1261\n",
            "test_41.jpg,2592,1944,Babushka,704,939,941,1274\n",
            "test_41.jpg,2592,1944,Phil,1092,741,1292,1123\n",
            "test_41.jpg,2592,1944,Cam,1357,994,1524,1246\n",
            "test_41.jpg,2592,1944,Sheila,1704,1030,1792,1194\n",
            "test_41.jpg,2592,1944,Capitan,1917,972,2204,1492\n",
            "test_41.jpg,2592,1944,Bob,2272,1034,2446,1332\n",
            "test_41.jpg,2592,1944,clock,2055,792,2279,1003\n",
            "test_17.jpg,2592,1944,bike,197,974,481,1423\n",
            "test_17.jpg,2592,1944,Babushka,866,937,1135,1323\n",
            "test_17.jpg,2592,1944,Nastya,1332,968,1424,1152\n",
            "test_17.jpg,2592,1944,Cam,1395,979,1646,1352\n",
            "test_17.jpg,2592,1944,Capitan,1637,990,1788,1221\n",
            "test_17.jpg,2592,1944,Sheila,1984,1023,2230,1479\n",
            "test_17.jpg,2592,1944,Bob,2284,1017,2501,1372\n",
            "test_17.jpg,2592,1944,Phil,1881,690,2179,1054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8"
      },
      "source": [
        "## Download base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDCj6ihgUMR",
        "outputId": "b8e15289-fbe3-4f9f-d139-2925f069eca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "#DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "print(MODEL,\"from\",MODEL_FILE)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 from ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGhvAObeiIix",
        "outputId": "61a647d6-4ba2-4b1c-a746-617f7cc6bdf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/pretrained_model\n",
            "total 24K\n",
            "drwxr-x---  4 345018 89939 4.0K Jul 11  2020 .\n",
            "drwxr-xr-x 25 root   root  4.0K Jan 28 14:48 ..\n",
            "drwxr-x---  2 345018 89939 4.0K Jul 10  2020 checkpoint\n",
            "-rw-r-----  1 345018 89939 4.5K Jul 11  2020 pipeline.config\n",
            "drwxr-x---  3 345018 89939 4.0K Jul 10  2020 saved_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnxlfRznPP3",
        "outputId": "a81a04ea-9243-4675-850d-17ae7691fcbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR+\"/checkpoint\", \"ckpt-0\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models/research/pretrained_model/checkpoint/ckpt-0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYW8J5JoLP4I"
      },
      "source": [
        "# Section 4: Transfer Learning Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD"
      },
      "source": [
        "## Configuring a Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhw7IdpLuiU"
      },
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join(DEST_DIR, 'pipeline.config')\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG1nCNpUXcRU"
      },
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtCbLF2i0wI"
      },
      "source": [
        "import re\n",
        "\n",
        "# training pipeline file defines:\n",
        "# - pretrain model path\n",
        "# - the train/test sets\n",
        "# - ID to Label mapping and number of classes\n",
        "# - training batch size\n",
        "# - epochs to trains\n",
        "# - learning rate\n",
        "# - etc\n",
        "\n",
        "# note we just need to use a sample one, and make edits to it.\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    s = s = re.sub(\n",
        "        '(fine_tune_checkpoint_type: \".*?)(classification)(.*?\")', 'fine_tune_checkpoint_type: \"{}\"'.format(\"detection\"), s, 1)\n",
        "\n",
        "    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test, we created earlier with our training/test sets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s, 1)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s, 1)\n",
        "\n",
        "    # label_map_path: ID to label file\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps (Number of epochs to train)\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "  \n",
        "    f.write(s)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IH96bbydOWn"
      },
      "source": [
        "#!cat {label_map_pbtxt_fname}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0MEEanocn6",
        "outputId": "02131944-134b-4527-ebd4-b2dbce7bd9f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# look for num_classes: 13, since we have 13 different objects \n",
        "!cat {pipeline_fname}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model {\n",
            "  ssd {\n",
            "    num_classes: 13\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 320\n",
            "        width: 320\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
            "      depth_multiplier: 1.0\n",
            "      min_depth: 16\n",
            "      conv_hyperparams {\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 3.9999998989515007e-05\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          random_normal_initializer {\n",
            "            mean: 0.0\n",
            "            stddev: 0.009999999776482582\n",
            "          }\n",
            "        }\n",
            "        activation: RELU_6\n",
            "        batch_norm {\n",
            "          decay: 0.996999979019165\n",
            "          scale: true\n",
            "          epsilon: 0.0010000000474974513\n",
            "        }\n",
            "      }\n",
            "      use_depthwise: true\n",
            "      override_base_feature_extractor_hyperparams: true\n",
            "      fpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        additional_layer_depth: 128\n",
            "      }\n",
            "    }\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        conv_hyperparams {\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 3.9999998989515007e-05\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              mean: 0.0\n",
            "              stddev: 0.009999999776482582\n",
            "            }\n",
            "          }\n",
            "          activation: RELU_6\n",
            "          batch_norm {\n",
            "            decay: 0.996999979019165\n",
            "            scale: true\n",
            "            epsilon: 0.0010000000474974513\n",
            "          }\n",
            "        }\n",
            "        depth: 128\n",
            "        num_layers_before_predictor: 4\n",
            "        kernel_size: 3\n",
            "        class_prediction_bias_init: -4.599999904632568\n",
            "        share_prediction_tower: true\n",
            "        use_depthwise: true\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        scales_per_octave: 2\n",
            "      }\n",
            "    }\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 9.99999993922529e-09\n",
            "        iou_threshold: 0.6000000238418579\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "        use_static_shapes: false\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    loss {\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          gamma: 2.0\n",
            "          alpha: 0.25\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "  }\n",
            "}\n",
            "train_config {\n",
            "  batch_size: 12\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_crop_image {\n",
            "      min_object_covered: 0.0\n",
            "      min_aspect_ratio: 0.75\n",
            "      max_aspect_ratio: 3.0\n",
            "      min_area: 0.75\n",
            "      max_area: 1.0\n",
            "      overlap_thresh: 0.0\n",
            "    }\n",
            "  }\n",
            "  sync_replicas: true\n",
            "  optimizer {\n",
            "    momentum_optimizer {\n",
            "      learning_rate {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: 0.07999999821186066\n",
            "          total_steps: 50000\n",
            "          warmup_learning_rate: 0.026666000485420227\n",
            "          warmup_steps: 1000\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.8999999761581421\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/checkpoint/ckpt-0\"\n",
            "  num_steps: 100\n",
            "  startup_delay_steps: 0.0\n",
            "  replicas_to_aggregate: 8\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  fine_tune_checkpoint_version: V2\n",
            "}\n",
            "train_input_reader {\n",
            "  label_map_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/label_map.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/train.record\"\n",
            "  }\n",
            "}\n",
            "eval_config {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "}\n",
            "eval_input_reader {\n",
            "  label_map_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/DeepPiCar/models/object_detection/data/annotations_hh/test.record\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23TECXvNezIF"
      },
      "source": [
        "## Run Tensorboard(Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2PZs-mSCmO",
        "outputId": "c4d3ee39-888b-4ead-9dcd-6700c7689bbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-28 14:48:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 52.202.168.65, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  58.4MB/s    in 0.2s    \n",
            "\n",
            "2023-01-28 14:48:18 (58.4 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8o6r1o5SC5M"
      },
      "source": [
        "# LOG_DIR = model_dir\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(LOG_DIR)\n",
        "# )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge1OX7gcSC7S"
      },
      "source": [
        "#get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5GSGxZNh8rp"
      },
      "source": [
        "### Get Tensorboard link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjhPT9iPSJ6T",
        "outputId": "520d97c1-495c-43b5-87c7-9f8397f97246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZc0RFcUjTa-"
      },
      "source": [
        "Now all inputs are set up, just train the model.   This process may take a few hours.   Since we are saving the model training results (model.ckpt-* files) in our google drive (a persistent storage that will survice the restart of our colab VM instance), we can safely leave and return a few hours later. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lvis"
      ],
      "metadata": {
        "id": "SfPUc3YutA4f",
        "outputId": "5fefc6d6-8789-467a-d21a-143d80301786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.8/dist-packages (0.5.3)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from lvis) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (2.8.2)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.8/dist-packages (from lvis) (3.2.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis) (4.6.0.66)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from lvis) (2.4.7)\n",
            "Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.8/dist-packages (from lvis) (0.29.33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDHjhKQofT5",
        "outputId": "58d58914-4866-4b93-ab12-dac78cdf3d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_steps = 20\n",
        "#SendEmail(\"Colab train started\")\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir='{model_dir}' \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "#SendEmail(\"Colab train finished\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-28 14:48:32.474609: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:48:32.474767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 14:48:32.474790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-01-28 14:48:37.669072: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "W0128 14:48:37.671522 140164563835968 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "I0128 14:48:37.702830 140164563835968 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 20\n",
            "I0128 14:48:37.710380 140164563835968 config_util.py:552] Maybe overwriting train_steps: 20\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0128 14:48:37.710740 140164563835968 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0128 14:48:37.814932 140164563835968 deprecation.py:350] From /content/models/research/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/DeepPiCar/models/object_detection/data/annotations_hh/train.record']\n",
            "I0128 14:48:37.837179 140164563835968 dataset_builder.py:162] Reading unweighted datasets: ['/content/DeepPiCar/models/object_detection/data/annotations_hh/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/DeepPiCar/models/object_detection/data/annotations_hh/train.record']\n",
            "I0128 14:48:37.837643 140164563835968 dataset_builder.py:79] Reading record datasets for input file: ['/content/DeepPiCar/models/object_detection/data/annotations_hh/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0128 14:48:37.837828 140164563835968 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0128 14:48:37.837924 140164563835968 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0128 14:48:37.852541 140164563835968 deprecation.py:350] From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0128 14:48:37.941552 140164563835968 deprecation.py:350] From /content/models/research/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "W0128 14:48:39.590133 140164563835968 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0128 14:48:56.898425 140164563835968 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0128 14:49:00.608167 140164563835968 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0128 14:49:02.471088 140164563835968 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2023-01-28 14:49:05.460591: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "2023-01-28 14:49:08.657634: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 60466176 exceeds 10% of free system memory.\n",
            "2023-01-28 14:49:08.658778: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 60466176 exceeds 10% of free system memory.\n",
            "2023-01-28 14:49:08.733281: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 60466176 exceeds 10% of free system memory.\n",
            "2023-01-28 14:49:08.764930: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 57264636 exceeds 10% of free system memory.\n",
            "2023-01-28 14:49:08.822611: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 45902880 exceeds 10% of free system memory.\n",
            "/usr/local/lib/python3.8/dist-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "2023-01-28 14:49:43.230168: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0128 14:49:44.506462 140162190513920 deprecation.py:554] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "INFO:tensorflow:Step 100 per-step time 7.826s\n",
            "I0128 15:02:46.705685 140164563835968 model_lib_v2.py:705] Step 100 per-step time 7.826s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.23488778,\n",
            " 'Loss/localization_loss': 0.16807063,\n",
            " 'Loss/regularization_loss': 0.15328555,\n",
            " 'Loss/total_loss': 0.55624396,\n",
            " 'learning_rate': 0.0319994}\n",
            "I0128 15:02:46.706299 140164563835968 model_lib_v2.py:708] {'Loss/classification_loss': 0.23488778,\n",
            " 'Loss/localization_loss': 0.16807063,\n",
            " 'Loss/regularization_loss': 0.15328555,\n",
            " 'Loss/total_loss': 0.55624396,\n",
            " 'learning_rate': 0.0319994}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-tUdtnRybs",
        "outputId": "c768fc29-7813-4118-f250-ab14a4e7c119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -ltra '{model_dir}'"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 30898\n",
            "drwx------ 2 root root     4096 Jan 22 12:16 fine_tuned_model\n",
            "-rw------- 1 root root    26246 Jan 24 06:49 ckpt-1.index\n",
            "-rw------- 1 root root 10516881 Jan 24 06:49 ckpt-1.data-00000-of-00001\n",
            "-rw------- 1 root root    26246 Jan 24 06:53 ckpt-2.index\n",
            "-rw------- 1 root root 10516881 Jan 24 06:53 ckpt-2.data-00000-of-00001\n",
            "drwx------ 2 root root     4096 Jan 28 14:49 train\n",
            "-rw------- 1 root root 10516881 Jan 28 14:49 ckpt-3.data-00000-of-00001\n",
            "-rw------- 1 root root    26246 Jan 28 14:49 ckpt-3.index\n",
            "-rw------- 1 root root      343 Jan 28 14:49 checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Kzh3_JLVW-"
      },
      "source": [
        "# Section 5: Save and Convert Model Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHoP90pUyKSq"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '%s/fine_tuned_model' % model_dir\n",
        "os.makedirs(output_directory, exist_ok=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikck2kvh_wTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7e091e-52cf-49b5-ecd5-7ba7392cc9fc"
      },
      "source": [
        "# lst = os.listdir(model_dir)\n",
        "# # find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
        "# lst = [l for l in lst if 'ckpt-' in l and '.index' in l]\n",
        "# #print(lst)\n",
        "# steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "# last_model = lst[steps.argmax()].replace('.index', '')\n",
        "# #last_model = lst[steps.argmax()]\n",
        "# last_model_path = os.path.join(model_dir, last_model)\n",
        "# print(last_model_path)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/DeepCar_Training/ckpt-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/models/research/object_detection/"
      ],
      "metadata": {
        "id": "mUd-xwGem0z8",
        "outputId": "73df7131-bf57-4d23-b649-767c8293069f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anchor_generators\t\t\t __init__.py\n",
            "box_coders\t\t\t\t inputs.py\n",
            "builders\t\t\t\t inputs_test.py\n",
            "colab_tutorials\t\t\t\t legacy\n",
            "configs\t\t\t\t\t matchers\n",
            "CONTRIBUTING.md\t\t\t\t meta_architectures\n",
            "core\t\t\t\t\t metrics\n",
            "data\t\t\t\t\t model_hparams.py\n",
            "data_decoders\t\t\t\t model_lib.py\n",
            "dataset_tools\t\t\t\t model_lib_tf1_test.py\n",
            "dockerfiles\t\t\t\t model_lib_tf2_test.py\n",
            "eval_util.py\t\t\t\t model_lib_v2.py\n",
            "eval_util_test.py\t\t\t model_main.py\n",
            "exporter_lib_tf2_test.py\t\t model_main_tf2.py\n",
            "exporter_lib_v2.py\t\t\t models\n",
            "exporter_main_v2.py\t\t\t model_tpu_main.py\n",
            "exporter.py\t\t\t\t packages\n",
            "exporter_tf1_test.py\t\t\t predictors\n",
            "export_inference_graph.py\t\t protos\n",
            "export_tflite_graph_lib_tf2.py\t\t __pycache__\n",
            "export_tflite_graph_lib_tf2_test.py\t README.md\n",
            "export_tflite_graph_tf2.py\t\t samples\n",
            "export_tflite_ssd_graph_lib.py\t\t test_data\n",
            "export_tflite_ssd_graph_lib_tf1_test.py  test_images\n",
            "export_tflite_ssd_graph.py\t\t tpu_exporters\n",
            "g3doc\t\t\t\t\t utils\n",
            "inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo exporting the model\n",
        "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --trained_checkpoint_dir='{model_dir}'"
      ],
      "metadata": {
        "id": "m1ILXsyVCrak",
        "outputId": "98f78001-9c82-457b-e34a-5a2e41fdb49b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creates the frozen inference graph in fine_tune_model\n",
            "2023-01-28 15:43:15.812867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 15:43:15.813003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 15:43:15.813021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-01-28 15:43:19.289868: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "W0128 15:43:27.447873 140695640226880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7ff5c4b134f0>, because it is not built.\n",
            "W0128 15:43:35.579466 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7ff5c4b134f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c48a0fa0>, because it is not built.\n",
            "W0128 15:43:35.932115 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c48a0fa0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4589d90>, because it is not built.\n",
            "W0128 15:43:35.932404 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4589d90>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c45897f0>, because it is not built.\n",
            "W0128 15:43:35.932536 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c45897f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c499a3d0>, because it is not built.\n",
            "W0128 15:43:35.932634 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c499a3d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48b1fd0>, because it is not built.\n",
            "W0128 15:43:35.932729 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48b1fd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c458fdc0>, because it is not built.\n",
            "W0128 15:43:35.932837 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c458fdc0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c46dc160>, because it is not built.\n",
            "W0128 15:43:35.932968 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c46dc160>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c49054c0>, because it is not built.\n",
            "W0128 15:43:35.933070 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c49054c0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a1f0d0>, because it is not built.\n",
            "W0128 15:43:35.933164 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a1f0d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c4a1f130>, because it is not built.\n",
            "W0128 15:43:35.933254 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7ff5c4a1f130>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4a1fa00>, because it is not built.\n",
            "W0128 15:43:35.933337 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4a1fa00>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a1f7c0>, because it is not built.\n",
            "W0128 15:43:35.933426 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a1f7c0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48e3430>, because it is not built.\n",
            "W0128 15:43:35.933517 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48e3430>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4990760>, because it is not built.\n",
            "W0128 15:43:35.933609 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4990760>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4657370>, because it is not built.\n",
            "W0128 15:43:35.933692 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4657370>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4657910>, because it is not built.\n",
            "W0128 15:43:35.933787 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4657910>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48691c0>, because it is not built.\n",
            "W0128 15:43:35.933880 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48691c0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4869e80>, because it is not built.\n",
            "W0128 15:43:35.933969 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4869e80>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4947f70>, because it is not built.\n",
            "W0128 15:43:35.934089 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4947f70>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4877d90>, because it is not built.\n",
            "W0128 15:43:35.934182 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4877d90>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4661430>, because it is not built.\n",
            "W0128 15:43:35.934273 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4661430>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4497b20>, because it is not built.\n",
            "W0128 15:43:35.934381 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4497b20>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4161430>, because it is not built.\n",
            "W0128 15:43:35.934484 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4161430>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4161700>, because it is not built.\n",
            "W0128 15:43:35.934571 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4161700>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4161e20>, because it is not built.\n",
            "W0128 15:43:35.934667 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4161e20>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4161d60>, because it is not built.\n",
            "W0128 15:43:35.934774 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4161d60>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c41619a0>, because it is not built.\n",
            "W0128 15:43:35.934863 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c41619a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c465da00>, because it is not built.\n",
            "W0128 15:43:35.934953 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c465da00>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4821fa0>, because it is not built.\n",
            "W0128 15:43:35.935046 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4821fa0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c49cf3d0>, because it is not built.\n",
            "W0128 15:43:35.935143 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c49cf3d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c485e9a0>, because it is not built.\n",
            "W0128 15:43:35.935236 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c485e9a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c485e580>, because it is not built.\n",
            "W0128 15:43:35.935327 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c485e580>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48bfa30>, because it is not built.\n",
            "W0128 15:43:35.935425 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48bfa30>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c489abb0>, because it is not built.\n",
            "W0128 15:43:35.935518 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c489abb0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48fadf0>, because it is not built.\n",
            "W0128 15:43:35.935638 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48fadf0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c47db9d0>, because it is not built.\n",
            "W0128 15:43:35.935733 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c47db9d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c49c6370>, because it is not built.\n",
            "W0128 15:43:35.935834 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c49c6370>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4896c10>, because it is not built.\n",
            "W0128 15:43:35.935922 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4896c10>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4a3abe0>, because it is not built.\n",
            "W0128 15:43:35.936033 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4a3abe0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a3a3a0>, because it is not built.\n",
            "W0128 15:43:35.936125 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a3a3a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4a3a340>, because it is not built.\n",
            "W0128 15:43:35.936216 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c4a3a340>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a3af40>, because it is not built.\n",
            "W0128 15:43:35.936299 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c4a3af40>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48ed340>, because it is not built.\n",
            "W0128 15:43:35.946649 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7ff5c48ed340>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c48edd30>, because it is not built.\n",
            "W0128 15:43:35.946950 140695640226880 save_impl.py:66] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7ff5c48edd30>, because it is not built.\n",
            "W0128 15:44:06.254827 140695640226880 save.py:271] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 173). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/DeepCar_Training/fine_tuned_model/saved_model/assets\n",
            "I0128 15:44:13.192985 140695640226880 builder_impl.py:797] Assets written to: /content/gdrive/My Drive/Colab Notebooks/DeepCar_Training/fine_tuned_model/saved_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlxqSTTgHMHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77a66b0-b52e-4541-99e7-5c36d6d6fa86"
      },
      "source": [
        "# !echo creates the frozen inference graph in fine_tune_model\n",
        "# # there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "# !python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "#     --input_type=image_tensor \\\n",
        "#     --pipeline_config_path={pipeline_fname} \\\n",
        "#     --output_directory='{output_directory}' \\\n",
        "#     --trained_checkpoint_prefix='{last_model_path}'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creates the frozen inference graph in fine_tune_model\n",
            "2023-01-28 15:02:51.707089: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 15:02:51.707441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-28 15:02:51.707484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "FATAL Flags parsing error: Unknown command line flag 'input_type'\n",
            "Pass --helpshort or --helpfull to see help on flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo convert saved model to TF-Lite\n",
        "#https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(f'{output_directory}/saved_model/') # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open(f'{output_directory}/saved_model/model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "cy5jsvIrFmdr",
        "outputId": "fba8108d-d3cb-4f10-acaa-ffae09cc87ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert saved model to TF-Lite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/gdrive/My\\ Drive/Colab\\ Notebooks/DeepCar_Training/fine_tuned_model/saved_model/"
      ],
      "metadata": {
        "id": "xxVqSir2HjXp",
        "outputId": "6ff2e8f4-99da-4ffd-df84-84e6eed2594d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 17M\n",
            "drwx------ 2 root root 4.0K Jan 28 15:44 assets\n",
            "-rw------- 1 root root   55 Jan 28 15:44 fingerprint.pb\n",
            "-rw------- 1 root root  10M Jan 28 16:00 model.tflite\n",
            "-rw------- 1 root root 6.5M Jan 28 15:44 saved_model.pb\n",
            "drwx------ 2 root root 4.0K Jan 28 15:44 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1gX19GlVW7"
      },
      "source": [
        "## Run inference test\n",
        "Test with images in repository `object_detection/data/images/test` directory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"models/object_detection/data/images_hh/test/\")"
      ],
      "metadata": {
        "id": "mOSEPybVPFDz"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_TEST_IMAGES_DIR"
      ],
      "metadata": {
        "id": "x3HWJJrCQKVV",
        "outputId": "a5635ea4-3a93-4e37-b53f-cf377bd8ba93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/DeepPiCar/models/object_detection/data/images_hh/test/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/DeepPiCar/models/object_detection/data/images_hh/test/"
      ],
      "metadata": {
        "id": "N4-CcG7GQM3e",
        "outputId": "00a3f375-cd68-4401-9ba6-9576649052d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_10.jpg\t    test_23.jpg\t\ttest_37.jpg\t    test_47.jpg\n",
            "test_10_labels.xml  test_23_labels.xml\ttest_37_labels.xml  test_47_labels.xml\n",
            "test_17.jpg\t    test_28.jpg\t\ttest_41.jpg\t    test_6.jpg\n",
            "test_17_lables.xml  test_28_labels.xml\ttest_41_labels.xml  test_6_labels.xml\n",
            "test_22.jpg\t    test_2.jpg\t\ttest_44.jpg\n",
            "test_22_labels.xml  test_2_labels.xml\ttest_44_labels.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "# Load saved model and build the detection function\n",
        "print('Loading model...', end='')\n",
        "start_time = time.time()\n",
        "detect_fn = tf.saved_model.load(f'{output_directory}/saved_model/')\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "metadata": {
        "id": "LUOUmABXJ9M0",
        "outputId": "0dcaf107-87e3-445e-bc27-3ad4b4d46507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...Done! Took 15.081169366836548 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(detect_fn.signatures.keys()))\n",
        "infer = detect_fn.signatures[\"serving_default\"]\n"
      ],
      "metadata": {
        "id": "m9oPY9loUQHH",
        "outputId": "b1b69853-06df-492e-e543-d46624f7fcec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['serving_default']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(detect_fn)"
      ],
      "metadata": {
        "id": "hL8FYUQ2S7Ha",
        "outputId": "1151e862-78a0-4efd-9915-876a4b5bf5e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7f7f1f4b8070>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
        "                                                                    use_display_name=True)"
      ],
      "metadata": {
        "id": "Y0m_aAXCMUq9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?detect_fn"
      ],
      "metadata": {
        "id": "ZiWOwQfKTsSM"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "directory = os.fsencode(PATH_TO_TEST_IMAGES_DIR)\n",
        "for input_file in os.listdir(directory):\n",
        "  filename = os.fsdecode(input_file)\n",
        "  if filename.endswith(\".jpg\"):\n",
        "    \n",
        "    print('Running inference for {}... '.format(filename), end='')\n",
        "    image_np = load_image_into_numpy_array(PATH_TO_TEST_IMAGES_DIR+'/'+filename)\n",
        "\n",
        "    converted_img  = tf.image.convert_image_dtype(image_np, tf.float32)[tf.newaxis, ...]\n",
        "    detections = infer(converted_img)\n",
        " \n",
        "    print(detections)\n",
        "\n",
        "    detections = {key:value.numpy() for key,value in detections.items()}\n",
        "    print(\"Found %d objects.\" % len(detections[\"detection_scores\"]))\n",
        "    image_with_boxes = draw_boxes(\n",
        "      image_np, detections[\"detection_boxes\"],\n",
        "      detections[\"detection_class_entities\"], detections[\"detection_scores\"])\n",
        "    display_image(image_with_boxes)\n",
        "\n",
        "\n",
        "    # # Things to try:\n",
        "    # # Flip horizontally\n",
        "    # # image_np = np.fliplr(image_np).copy()\n",
        "\n",
        "    # # Convert image to grayscale\n",
        "    # # image_np = np.tile(\n",
        "    # #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "    # # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    # input_tensor = tf.convert_to_tensor(image_np)\n",
        "    # # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    # input_tensor = input_tensor[tf.newaxis, ...]\n",
        "    # #print(input_tensor)\n",
        "    # #input_tensor = np.expand_dims(image_np, 0)\n",
        "    # #detections = infer(input_tensor)\n",
        "    # detections = infer(tf.constant(input_tensor))['output']\n",
        "\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                   for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_np_with_detections,\n",
        "          detections['detection_boxes'],\n",
        "          detections['detection_classes'],\n",
        "          detections['detection_scores'],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=200,\n",
        "          min_score_thresh=.30,\n",
        "          agnostic_mode=False)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    print('Done')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0jzms4WOO7dC",
        "outputId": "3bef23a5-1ee0-41e6-c3b7-befb761d687e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference for test_6.jpg... {'output_0': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'output_1': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'output_3': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'output_2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-49e18b917398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found %d objects.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"detection_scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     image_with_boxes = draw_boxes(\n\u001b[1;32m     38\u001b[0m       \u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"detection_boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'detection_scores'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzj9A4e5mj5l",
        "outputId": "415a532c-7b96-4fe9-d8a5-a993bfd458c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = f'{output_directory}/saved_model/'\n",
        "pb_fname = f'{PATH_TO_CKPT}/saved_model.pb'\n",
        "print(PATH_TO_CKPT)\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"models/object_detection/data/images_hh/test\")\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/DeepCar_Training/fine_tuned_model/saved_model/\n",
            "['/content/DeepPiCar/models/object_detection/data/images_hh/test/test_6.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_37.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_28.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_47.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_22.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_17.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_10.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_2.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_23.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_41.jpg', '/content/DeepPiCar/models/object_detection/data/images_hh/test/test_44.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5YUMdg1Po7",
        "outputId": "73d1a3f6-9130-4a6e-a16c-26ed441a77f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.compat.v1.GraphDef()\n",
        "    with tf.compat.v2.io.gfile.GFile(pb_fname, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-3eec6114d99c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpb_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mserialized_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mod_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mod_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDecodeError\u001b[0m: Error parsing message with type 'tensorflow.GraphDef'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEmFi1DFGBQ8"
      },
      "source": [
        "# running inferences.  This should show images with bounding boxes\n",
        "%matplotlib inline\n",
        "\n",
        "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=2)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBbsZRWt0h6l"
      },
      "source": [
        "## Convert to Edge TPU's tflite Format  \n",
        "The only known way, at time of writing (April 2019), is to download the below quantized tflite file from above, and use [Google's web compiler](https://coral.withgoogle.com/web-compiler/) to convert to Edge TPU's tflite format.   Unfortunately, this step has to be done by hand, and NOT via a script.  \n",
        "\n",
        "Here are the requirements of Edge TPU web compiler.  If you have followed the above steps closely, you have met these requirements.\n",
        "\n",
        "- Tensor parameters are quantized (8-bit fixed-point numbers). You must use quantization-aware training (post-training quantization is not supported).   (this is why we are using `ssd_mobilenet_v2_quantized` base model and not the  `ssd_mobilenet_v2` base model   \n",
        "- Tensor sizes are constant at compile-time (no dynamic sizes).\n",
        "- Model parameters (such as bias tensors) are constant at compile-time.\n",
        "- Tensors are either 1-, 2-, or 3-dimensional. If a tensor has more than 3 dimensions, then only the 3 innermost dimensions may have a size greater than 1.\n",
        "- The model uses only the operations supported by the Edge TPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HhQ9y_Jcrfa"
      },
      "source": [
        "# download this file from google drive.\n",
        "!ls -lt '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/road_signs_quantized.tflite'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WzRVnDj0jt9"
      },
      "source": [
        "Wait for about 1-2 minutes for compilation to finish.  And we can download the model file as `road_signs_quantized_edgetpu.tflite`.  This is the file you need to copy to raspberry pi with TPU to run object detection.\n",
        "\n",
        "We are all done with colab notebook training, now time to switch back to raspberry pi, and run `~/DeepPiCar/models/object_detection/code/object_detection_usb.py`.  You should see a video feed where road sign and persons are boxed with confidence level around them.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG852pD0Buaq"
      },
      "source": [
        "```bash\n",
        "# make sure the the road_signs_quantized_edgetpu.tflite is in the right directory in your pi\n",
        "pi@raspberrypi:~/DeepPiCar/models/object_detection/data/model_result $ ls -ltr\n",
        "total 10040\n",
        "-rw-r--r-- 1 pi pi      97 Apr 15 01:01 road_sign_labels.txt\n",
        "-rw-r--r-- 1 pi pi 4793504 Apr 16 15:49 road_signs_quantized.tflite\n",
        "-rw-r--r-- 1 pi pi 5478080 Apr 16 15:49 road_signs_quantized_edgetpu.tflite\n",
        "\n",
        "pi@raspberrypi:~/DeepPiCar/models/object_detection $ python3 code/object_detection_usb.py\n",
        "\n",
        "------\n",
        "2019-04-16 16:22:28.489224: 13.49 FPS, 74.12ms total, 70.84ms in tf \n",
        "Green Traffic Light, 80% [[240.61578751 131.68985367]\n",
        " [287.21975327 195.79172134]] 60.42ms\n",
        "Stop Sign, 44% [[  0.         305.1651001 ]\n",
        " [180.84949493 409.32563782]] 60.42ms\n",
        "\n",
        "------\n",
        "2019-04-16 16:22:28.618309: 14.83 FPS, 67.44ms total, 60.42ms in tf \n",
        "Person, 89% [[505.6583786  279.52325821]\n",
        " [530.85933685 360.0169754 ]] 62.54ms\n",
        "Green Traffic Light, 72% [[237.96649933 130.58757782]\n",
        " [283.52127075 203.24180603]] 62.54ms\n",
        "Red Traffic Light, 62% [[283.23583603 169.27398682]\n",
        " [330.91316223 269.20692444]] 62.54ms\n",
        "Stop Sign, 56% [[ 51.01628304 165.80377579]\n",
        " [101.48646355 227.05183029]] 62.54ms\n",
        "Person, 44% [[396.8661499  298.65327835]\n",
        " [468.4034729  422.04421997]] 62.54ms\n",
        "------\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqSFdHQvElks"
      },
      "source": [
        "# Section 6: Last Words on this Project\n",
        "\n",
        "Of course, depending many factors, not all objects in the video frame will be identified.  This is the chance to improve your model.   Try to train longer, or train with more labeled images, or augment your existing images with different zooms/rotations/contrast/lighting.  In my case, I started with one camera, which was somewhat fuzzy, and precision low.  When I switched to a HD camera, the model precision was significantly better.   This was just another way.\n",
        "\n",
        "It is awesome that for just over $100 in hardware, we can do real time object detection at home.   Moreover, thanks to Google, all you need is a browser to train this huge model!  Having fun with your own raspberry pi object detection projects!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eswamFiJLrnM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}